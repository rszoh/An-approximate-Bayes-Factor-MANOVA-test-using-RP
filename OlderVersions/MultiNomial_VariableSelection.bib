% Encoding: UTF-8

@Article{Cai2013,
  author     = {Tony Cai and Weidong Liu and Yin Xia},
  title      = {Two-Sample Covariance Matrix Testing and Support Recovery in High-Dimensional and Sparse Settings},
  journal    = {Journal of the American Statistical Association},
  year       = {2013},
  volume     = {108},
  number     = {501},
  pages      = {265--277},
  month      = {mar},
  bdsk-url-1 = {https://doi.org/10.1080/01621459.2012.758041},
  doi        = {10.1080/01621459.2012.758041},
  publisher  = {Informa {UK} Limited},
}

@Article{Srivastava2014,
  author     = {Muni S. Srivastava and Hirokazu Yanagihara and Tatsuya Kubokawa},
  title      = {Tests for covariance matrices in high dimension with less sample size},
  journal    = {Journal of Multivariate Analysis},
  year       = {2014},
  volume     = {130},
  pages      = {289--309},
  month      = {sep},
  bdsk-url-1 = {https://doi.org/10.1016/j.jmva.2014.06.003},
  doi        = {10.1016/j.jmva.2014.06.003},
  publisher  = {Elsevier {BV}},
}

@Article{Li2012,
  author     = {Jun Li and Song Xi Chen},
  title      = {Two sample tests for high-dimensional covariance matrices},
  journal    = {The Annals of Statistics},
  year       = {2012},
  volume     = {40},
  number     = {2},
  pages      = {908--940},
  month      = {apr},
  bdsk-url-1 = {https://doi.org/10.1214/12-aos993},
  doi        = {10.1214/12-aos993},
  publisher  = {Institute of Mathematical Statistics},
}

@Article{Ma2015,
  author        = {Yingying Ma and Wei Lan and Hansheng Wang},
  title         = {A high dimensional two-sample test under a low dimensional factor structure},
  journal       = {Journal of Multivariate Analysis},
  year          = {2015},
  volume        = {140},
  pages         = {162--170},
  month         = {sep},
  __markedentry = {[rszoh:]},
  bdsk-url-1    = {https://doi.org/10.1016/j.jmva.2015.05.005},
  doi           = {10.1016/j.jmva.2015.05.005},
  publisher     = {Elsevier {BV}},
}

@Article{Bai2009,
  author     = {Zhidong Bai and Dandan Jiang and Jian-Feng Yao and Shurong Zheng},
  title      = {Corrections to {LRT} on large-dimensional covariance matrix by {RMT}},
  journal    = {The Annals of Statistics},
  year       = {2009},
  volume     = {37},
  number     = {6B},
  pages      = {3822--3840},
  month      = {dec},
  bdsk-url-1 = {https://doi.org/10.1214/09-aos694},
  doi        = {10.1214/09-aos694},
  publisher  = {Institute of Mathematical Statistics},
}

@Article{Jiang2012,
  author     = {Dandan Jiang and Tiefeng Jiang and Fan Yang},
  title      = {Likelihood ratio tests for covariance matrices of high-dimensional normal distributions},
  journal    = {Journal of Statistical Planning and Inference},
  year       = {2012},
  volume     = {142},
  number     = {8},
  pages      = {2241--2256},
  month      = {aug},
  bdsk-url-1 = {https://doi.org/10.1016/j.jspi.2012.02.057},
  doi        = {10.1016/j.jspi.2012.02.057},
  publisher  = {Elsevier {BV}},
}

@Article{Carvalho2010,
  author     = {C. M. Carvalho and N. G. Polson and J. G. Scott},
  title      = {The horseshoe estimator for sparse signals},
  journal    = {Biometrika},
  year       = {2010},
  volume     = {97},
  number     = {2},
  pages      = {465--480},
  month      = {apr},
  bdsk-url-1 = {https://doi.org/10.1093/biomet/asq017},
  doi        = {10.1093/biomet/asq017},
  publisher  = {Oxford University Press ({OUP})},
}

@Article{Pas2014,
  author     = {S. L. van der Pas and B. J. K. Kleijn and A. W. van der Vaart},
  title      = {The horseshoe estimator: Posterior concentration around nearly black vectors},
  journal    = {Electronic Journal of Statistics},
  year       = {2014},
  volume     = {8},
  number     = {2},
  pages      = {2585--2618},
  bdsk-url-1 = {https://doi.org/10.1214/14-ejs962},
  doi        = {10.1214/14-ejs962},
  publisher  = {Institute of Mathematical Statistics},
}

@Article{Datta2013,
  author     = {Jyotishka Datta and Jayanta. K. Ghosh},
  title      = {Asymptotic Properties of Bayes Risk for the Horseshoe Prior},
  journal    = {Bayesian Analysis},
  year       = {2013},
  volume     = {8},
  number     = {1},
  pages      = {111--132},
  month      = {mar},
  bdsk-url-1 = {https://doi.org/10.1214/13-ba805},
  doi        = {10.1214/13-ba805},
  publisher  = {Institute of Mathematical Statistics},
}

@Article{Cecco2017,
  author     = {Loris De Cecco and Marco Giannoccaro and Edoardo Marchesi and Paolo Bossi and Federica Favales and Laura Locati and Lisa Licitra and Silvana Pilotti and Silvana Canevari},
  title      = {Integrative {miRNA}-Gene Expression Analysis Enables Refinement of Associated Biology and Prediction of Response to Cetuximab in Head and Neck Squamous Cell Cancer},
  journal    = {Genes},
  year       = {2017},
  volume     = {8},
  number     = {1},
  pages      = {35},
  month      = {jan},
  bdsk-url-1 = {https://doi.org/10.3390/genes8010035},
  doi        = {10.3390/genes8010035},
  publisher  = {{MDPI} {AG}},
}

@Article{Chen2012,
  author     = {Lisha Chen and Jianhua Z. Huang},
  title      = {Sparse Reduced-Rank Regression for Simultaneous Dimension Reduction and Variable Selection},
  journal    = {Journal of the American Statistical Association},
  year       = {2012},
  volume     = {107},
  number     = {500},
  pages      = {1533--1545},
  month      = {oct},
  bdsk-url-1 = {https://doi.org/10.1080/01621459.2012.734178},
  doi        = {10.1080/01621459.2012.734178},
  publisher  = {Informa {UK} Limited},
}

@Article{Chakraborty2016,
  author      = {Antik Chakraborty and Anirban Bhattacharya and Bani K. Mallick},
  title       = {Bayesian sparse multiple regression for simultaneous rank reduction and variable selection},
  abstract    = {We develop a Bayesian methodology aimed at simultaneously estimating low-rank and row-sparse matrices in a high-dimensional multiple-response linear regression model. We consider a carefully devised shrinkage prior on the matrix of regression coefficients which obviates the need to specify a prior on the rank, and shrinks the regression matrix towards low-rank and row-sparse structures. We provide theoretical support to the proposed methodology by proving minimax optimality of the posterior mean under the prediction risk in ultra-high dimensional settings where the number of predictors can grow sub-exponentially relative to the sample size. A one-step post-processing scheme induced by group lasso penalties on the rows of the estimated coefficient matrix is proposed for variable selection, with default choices of tuning parameters. We additionally provide an estimate of the rank using a novel optimization function achieving dimension reduction in the covariate space. We exhibit the performance of the proposed methodology in an extensive simulation study and a real data example.},
  date        = {2016-12-02},
  eprint      = {1612.00877v3},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1612.00877v3:PDF},
  keywords    = {stat.ME, math.ST, stat.TH},
}

@Article{Mukhopadhyay2017,
  author      = {Minerva Mukhopadhyay and David B. Dunson},
  title       = {Targeted Random Projection for Prediction from High-Dimensional Features},
  abstract    = {We consider the problem of computationally-efficient prediction from high dimensional and highly correlated predictors in challenging settings where accurate variable selection is effectively impossible. Direct application of penalization or Bayesian methods implemented with Markov chain Monte Carlo can be computationally daunting and unstable. Hence, some type of dimensionality reduction prior to statistical analysis is in order. Common solutions include application of screening algorithms to reduce the regressors, or dimension reduction using projections of the design matrix. The former approach can be highly sensitive to threshold choice in finite samples, while the later can have poor performance in very high-dimensional settings. We propose a TArgeted Random Projection (TARP) approach that combines positive aspects of both strategies to boost performance. In particular, we propose to use information from independent screening to order the inclusion probabilities of the features in the projection matrix used for dimension reduction, leading to data-informed sparsity. We provide theoretical support for a Bayesian predictive algorithm based on TARP, including both statistical and computational complexity guarantees. Examples for simulated and real data applications illustrate gains relative to a variety of competitors.},
  date        = {2017-12-06},
  eprint      = {1712.02445v1},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1712.02445v1:PDF},
  keywords    = {math.ST, stat.ME, stat.TH},
}

@Article{Chen2011,
  author     = {Kun Chen and Kung-Sik Chan and Nils Chr. Stenseth},
  title      = {Reduced rank stochastic regression with a sparse singular value decomposition},
  journal    = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year       = {2011},
  volume     = {74},
  number     = {2},
  pages      = {203--221},
  month      = {nov},
  bdsk-url-1 = {https://doi.org/10.1111/j.1467-9868.2011.01002.x},
  doi        = {10.1111/j.1467-9868.2011.01002.x},
  publisher  = {Wiley},
}

@Article{Chen2012a,
  author        = {Lisha Chen and Jianhua Z. Huang},
  title         = {Sparse Reduced-Rank Regression for Simultaneous Dimension Reduction and Variable Selection},
  journal       = {Journal of the American Statistical Association},
  year          = {2012},
  volume        = {107},
  number        = {500},
  pages         = {1533--1545},
  month         = {oct},
  __markedentry = {[rszoh:6]},
  bdsk-url-1    = {https://doi.org/10.1080/01621459.2012.734178},
  doi           = {10.1080/01621459.2012.734178},
  publisher     = {Informa {UK} Limited},
}

@InCollection{Thanei2017,
  author     = {Gian-Andrea Thanei and Christina Heinze and Nicolai Meinshausen},
  title      = {Random Projections for Large-Scale Regression},
  booktitle  = {Contributions to Statistics},
  publisher  = {Springer International Publishing},
  year       = {2017},
  pages      = {51--68},
  bdsk-url-1 = {https://doi.org/10.1007/978-3-319-41573-4_3},
  doi        = {10.1007/978-3-319-41573-4_3},
}

@Article{Izenman1975,
  author     = {Alan Julian Izenman},
  title      = {Reduced-rank regression for the multivariate linear model},
  journal    = {Journal of Multivariate Analysis},
  year       = {1975},
  volume     = {5},
  number     = {2},
  pages      = {248--264},
  month      = {jun},
  bdsk-url-1 = {https://doi.org/10.1016/0047-259x(75)90042-1},
  doi        = {10.1016/0047-259x(75)90042-1},
  publisher  = {Elsevier {BV}},
}

@Book{Izenman2008,
  title      = {Modern Multivariate Statistical Techniques},
  publisher  = {Springer New York},
  year       = {2008},
  author     = {Alan J. Izenman},
  bdsk-url-1 = {https://doi.org/10.1007/978-0-387-78189-1},
  doi        = {10.1007/978-0-387-78189-1},
}

@Article{Li2015,
  author     = {Yanming Li and Bin Nan and Ji Zhu},
  title      = {Multivariate sparse group lasso for the multivariate multiple linear regression with an arbitrary group structure},
  journal    = {Biometrics},
  year       = {2015},
  volume     = {71},
  number     = {2},
  pages      = {354--363},
  month      = {mar},
  bdsk-url-1 = {https://doi.org/10.1111/biom.12292},
  doi        = {10.1111/biom.12292},
  publisher  = {Wiley},
}

@Article{Anderson1951,
  author     = {T. W. Anderson},
  title      = {Estimating Linear Restrictions on Regression Coefficients for Multivariate Normal Distributions},
  journal    = {The Annals of Mathematical Statistics},
  year       = {1951},
  volume     = {22},
  number     = {3},
  pages      = {327--351},
  month      = {sep},
  bdsk-url-1 = {https://doi.org/10.1214/aoms/1177729580},
  doi        = {10.1214/aoms/1177729580},
  publisher  = {Institute of Mathematical Statistics},
}

@Article{Paruolo2000,
  author     = {Paolo Paruolo and Gregory C. Reinsel and Raja P. Velu},
  title      = {Multivariate Reduced Rank Regression, Theory and Applications},
  journal    = {Journal of the American Statistical Association},
  year       = {2000},
  volume     = {95},
  number     = {450},
  pages      = {683},
  month      = {jun},
  bdsk-url-1 = {https://doi.org/10.2307/2669425},
  doi        = {10.2307/2669425},
  publisher  = {{JSTOR}},
}

@Book{velu1998multivariate,
  title     = {Multivariate reduced-rank regression: theory and applications},
  publisher = {Springer Science \& Business Media},
  year      = {1998},
  author    = {Velu, Raja and Reinsel, Gregory C},
  volume    = {136},
}

@Article{Yuan2007,
  author     = {Ming Yuan and Ali Ekici and Zhaosong Lu and Renato Monteiro},
  title      = {Dimension reduction and coefficient estimation in multivariate linear regression},
  journal    = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year       = {2007},
  volume     = {69},
  number     = {3},
  pages      = {329--346},
  month      = {jun},
  bdsk-url-1 = {https://doi.org/10.1111/j.1467-9868.2007.00591.x},
  doi        = {10.1111/j.1467-9868.2007.00591.x},
  publisher  = {Wiley},
}

@Article{Bunea2012,
  author     = {Florentina Bunea and Yiyuan She and Marten H. Wegkamp},
  title      = {Joint variable and rank selection for parsimonious estimation of high-dimensional matrices},
  journal    = {The Annals of Statistics},
  year       = {2012},
  volume     = {40},
  number     = {5},
  pages      = {2359--2388},
  month      = {oct},
  bdsk-url-1 = {https://doi.org/10.1214/12-aos1039},
  doi        = {10.1214/12-aos1039},
  publisher  = {Institute of Mathematical Statistics},
}

@Article{Li2012a,
  author    = {Runze Li and Wei Zhong and Liping Zhu},
  title     = {Feature Screening via Distance Correlation Learning},
  journal   = {Journal of the American Statistical Association},
  year      = {2012},
  volume    = {107},
  number    = {499},
  pages     = {1129--1139},
  month     = {jun},
  doi       = {10.1080/01621459.2012.695654},
  publisher = {Informa {UK} Limited},
}

@Article{Szekely2007,
  author    = {G{\'{a}}bor J. Sz{\'{e}}kely and Maria L. Rizzo and Nail K. Bakirov},
  title     = {Measuring and testing dependence by correlation of distances},
  journal   = {The Annals of Statistics},
  year      = {2007},
  volume    = {35},
  number    = {6},
  pages     = {2769--2794},
  month     = {dec},
  doi       = {10.1214/009053607000000505},
  publisher = {Institute of Mathematical Statistics},
}

@Article{BAI2018157,
  author        = {Ray Bai and Malay Ghosh},
  title         = {High-dimensional multivariate posterior consistency under global--local shrinkage priors},
  journal       = {Journal of Multivariate Analysis},
  year          = {2018},
  volume        = {167},
  pages         = {157 - 170},
  issn          = {0047-259X},
  abstract      = {We consider sparse Bayesian estimation in the classical multivariate linear regression model with p regressors and q response variables. In univariate Bayesian linear regression with a single response y, shrinkage priors which can be expressed as scale mixtures of normal densities are popular for obtaining sparse estimates of the coefficients. In this paper, we extend the use of these priors to the multivariate case to estimate a pÃ—q coefficients matrix B. We derive sufficient conditions for posterior consistency under the Bayesian multivariate linear regression framework and prove that our method achieves posterior consistency even when p>n and even when p grows at nearly exponential rate with the sample size. We derive an efficient Gibbs sampling algorithm and provide the implementation in a comprehensive R package called MBSP. Finally, we demonstrate through simulations and data analysis that our model has excellent finite sample performance.},
  bdsk-url-1    = {http://www.sciencedirect.com/science/article/pii/S0047259X17306905},
  bdsk-url-2    = {https://doi.org/10.1016/j.jmva.2018.04.010},
  date-added    = {2018-09-29 20:12:15 -0500},
  date-modified = {2018-09-29 20:12:15 -0500},
  doi           = {https://doi.org/10.1016/j.jmva.2018.04.010},
  keywords      = {Heavy tail, High-dimensional data, Posterior consistency, Shrinkage estimation, Sparsity, Variable selection},
  url           = {http://www.sciencedirect.com/science/article/pii/S0047259X17306905},
}

@Article{GOH201714,
  author        = {Gyuhyeong Goh and Dipak K. Dey and Kun Chen},
  title         = {Bayesian sparse reduced rank multivariate regression},
  journal       = {Journal of Multivariate Analysis},
  year          = {2017},
  volume        = {157},
  pages         = {14 - 28},
  issn          = {0047-259X},
  abstract      = {Many modern statistical problems can be cast in the framework of multivariate regression, where the main task is to make statistical inference for a possibly sparse and low-rank coefficient matrix. The low-rank structure in the coefficient matrix is of intrinsic multivariate nature, which, when combined with sparsity, can further lift dimension reduction, conduct variable selection, and facilitate model interpretation. Using a Bayesian approach, we develop a unified sparse and low-rank multivariate regression method to both estimate the coefficient matrix and obtain its credible region for making inference. The newly developed sparse and low-rank prior for the coefficient matrix enables rank reduction, predictor selection and response selection simultaneously. We utilize the marginal likelihood to determine the regularization hyperparameter, so our method maximizes its posterior probability given the data. For theoretical aspect, the posterior consistency is established to discuss an asymptotic behavior of the proposed method. The efficacy of the proposed approach is demonstrated via simulation studies and a real application on yeast cell cycle data.},
  bdsk-url-1    = {http://www.sciencedirect.com/science/article/pii/S0047259X17301112},
  bdsk-url-2    = {https://doi.org/10.1016/j.jmva.2017.02.007},
  date-added    = {2018-09-29 20:15:06 -0500},
  date-modified = {2018-09-29 20:15:06 -0500},
  doi           = {https://doi.org/10.1016/j.jmva.2017.02.007},
  keywords      = {Bayesian, Low rank, Posterior consistency, Rank reduction, Sparsity},
  url           = {http://www.sciencedirect.com/science/article/pii/S0047259X17301112},
}

@Article{GEWEKE1996121,
  author        = {John Geweke},
  title         = {Bayesian reduced rank regression in econometrics},
  journal       = {Journal of Econometrics},
  year          = {1996},
  volume        = {75},
  number        = {1},
  pages         = {121 - 146},
  issn          = {0304-4076},
  abstract      = {The reduced rank regression model arises repeatedly in theoretical and applied econometrics. To date the only general treatment of this model have been frequentist. This paper develops general methods for Bayesian inference with noninformative reference priors in this model, based on a Markov chain sampling algorithm, and procedures for obtaining predictive odds ratios for regression models with different ranks. These methods are used to obtain evidence on the number of factors in a capital asset pricing model.},
  bdsk-url-1    = {http://www.sciencedirect.com/science/article/pii/0304407695017739},
  bdsk-url-2    = {https://doi.org/10.1016/0304-4076(95)01773-9},
  date-added    = {2018-09-29 20:17:05 -0500},
  date-modified = {2018-09-29 20:17:05 -0500},
  doi           = {https://doi.org/10.1016/0304-4076(95)01773-9},
  keywords      = {Factor model, Predictive odds, Capital asset pricing model},
  url           = {http://www.sciencedirect.com/science/article/pii/0304407695017739},
}

@Article{Szekely2009,
  author    = {G{\'{a}}bor J. Sz{\'{e}}kely and Maria L. Rizzo},
  title     = {Brownian distance covariance},
  journal   = {The Annals of Applied Statistics},
  year      = {2009},
  volume    = {3},
  number    = {4},
  pages     = {1236--1265},
  month     = {dec},
  doi       = {10.1214/09-aoas312},
  publisher = {Institute of Mathematical Statistics},
}

@Article{sattler2019testing,
  author  = {Sattler, Paavo and Bathke, Arne C and Pauly, Markus},
  title   = {Testing Hypotheses about Covariance Matrices in General MANOVA Designs},
  journal = {arXiv preprint arXiv:1909.06205},
  year    = {2019},
}

@Article{ahmad2019multiple,
  author    = {Ahmad, M Rauf},
  title     = {Multiple comparisons of mean vectors with large dimension under general conditions},
  journal   = {Journal of Statistical Computation and Simulation},
  year      = {2019},
  volume    = {89},
  number    = {6},
  pages     = {1044--1059},
  publisher = {Taylor \& Francis},
}

@Article{schott2007some,
  author    = {Schott, James R},
  title     = {Some high-dimensional tests for a one-way MANOVA},
  journal   = {Journal of Multivariate Analysis},
  year      = {2007},
  volume    = {98},
  number    = {9},
  pages     = {1825--1839},
  publisher = {Elsevier},
}
@article{cai2014high,
  title={High-dimensional sparse MANOVA},
  author={Cai, T Tony and Xia, Yin},
  journal={Journal of Multivariate Analysis},
  volume={131},
  pages={174--196},
  year={2014},
  publisher={Elsevier}
}
@article{lin2020high,
  title={High-dimensional MANOVA via Bootstrapping and its Application to Functional Data and Sparse Count Data},
  author={Lin, Zhenhua and Lopes, Miles E and M{\"u}ller, Hans-Georg},
  journal={arXiv preprint arXiv:2007.01058},
  year={2020}
}
@article{ahmad2019unified,
  title={A unified approach to testing mean vectors with large dimensions},
  author={Ahmad, M Rauf},
  journal={AStA Advances in Statistical Analysis},
  volume={103},
  number={4},
  pages={593--618},
  year={2019},
  publisher={Springer}
}
@article{fujikoshi2004asymptotic,
  title={Asymptotic results of a high dimensional MANOVA test and power comparison when the dimension is large compared to the sample size},
  author={Fujikoshi, Yasunori and Himeno, Tetsuto and Wakaki, Hirofumi},
  journal={Journal of the Japan Statistical Society},
  volume={34},
  number={1},
  pages={19--26},
  year={2004},
  publisher={THE JAPAN STATISTICAL SOCIETY}
}
@Comment{jabref-meta: databaseType:bibtex;}
