\documentclass[12pt]{article}
\usepackage{amsthm}
%%%%% PLACE YOUR OWN MACROS HERE %%%%%
\usepackage{verbatim,color,amssymb}
\usepackage{amsmath}					
\usepackage{amsthm}					
%\usepackage{algorithm,algorithmic}
\usepackage[round]{natbib}
%\usepackage{cite}
\usepackage{setspace}
%\usepackage[mathscr]{euscript}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{lineno}
%\usepackage[compact]{titlesec}
\usepackage{listings}
\usepackage{rotating}
\usepackage{subfig,subfloat}
%\usepackage{multirow}
%\usepackage{lineno}
\usepackage{booktabs}
\def\rot{\rotatebox}
\usepackage{mathtools}
\usepackage{subfig}
\usepackage{caption}
\usepackage{xr}
%\usepackage{kbordermatrix}% http://www.hss.caltech.edu/~kcb/TeX/kbordermatrix.sty
%\renewcommand{\kbldelim}{(}% Left delimiter
%\renewcommand{\kbrdelim}{)}% Right delimiter
%\captionsetup[subfigure]{labelformat=parens,
%	labelsep=space,
%	font=small,
%	margin=0em
%}
\usepackage{float}
\newsubfloat{figure}% Allow sub-figures
\usepackage{tikz}
\usetikzlibrary{arrows,chains,backgrounds,fit}
\usepackage{multirow}
\usepackage{lineno}

\def\pdfshellescape{1}

\setlength{\textheight}{9in}
\setlength{\textwidth}{6in}
\setlength{\topmargin}{-36pt}
\setlength{\oddsidemargin}{15pt}
\setlength{\evensidemargin}{0pt}
\tolerance=500
\renewcommand{\baselinestretch}{1.5}


%\usepackage[english]{babel}
%\usepackage[utf8]{inputenc}
%\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}

%%%%%%%%%%%%%%%
% Begin New Definitions  %%
%%%%%%%%%%%%%%%


\newtheorem{Th}{\underline{\bf Theorem}}
\newtheorem{Assmp}{{\bf Assumption}}
\newtheorem{Cond}{\underline{\bf Conditions}}
\newtheorem{Proof}{Proof}
\newtheorem*{Proof*}{Proof}
\newtheorem{Mth}{Main Theorem}
\newtheorem{Def}{Definition}
\newtheorem{Rem}{\underline{\bf Remark}}
\newtheorem{Qes}{Question}
\newtheorem{Prop}{Proposition}
\newtheorem{Lem}{\underline{\bf Lemma}}
\newtheorem{Cor}{\underline{\bf Corollary}}
\newtheorem{Exa}{Example}
\newtheorem{Eq}{Equation}

\newcommand{\MyProof}{\noindent\textbf{Proof. }}
\def\bzero{{\mathbf 0}}
\newcommand{\uzero}            {\mbox{\boldmath$0$}}
\newcommand{\uone}               {\mbox{\boldmath$1$}}
\def\etal{\emph{et al.}}

\def\nN{\mathbb{N}}
\def\rR{\mathbb{R}}
\def\eE{\mathbb{E}}

\def\L{{\cal L}}
\def\B{{\cal B}}
\def\C{{\cal C}}
\def\D{{\cal D}}
\def\E{{\cal E}}
\def\F{{\cal F}}
\def\G{{\cal G}}
\def\K{{\cal K}}
\def\M{{\cal M}}
\def\N{{\cal N}}
\def\calP{{\cal P}}
\def\S{{\cal S}}
\def\T{{\cal T}}
\def\U{{\cal U}}
\def\W{{\cal W}}
\def\V{{\cal V}}
\def\X{{\cal X}}
\def\Z{{\cal Z}}
\def\Y{{\cal Y}}
\def\sumi{\sum_{i=1}^n}

\def\scrC{{\mathscr{C}}}


\def\diag{\hbox{diag}}
\def\Ind{\hbox{I}}
\def\wh{\widehat}
\def\wt{\widetilde}
%\def\wb{\breve}
\def\AIC{\hbox{AIC}}
\def\BIC{\hbox{BIC}}
\def\diag{\hbox{diag}}
\def\log{\hbox{log}}
\def\bias{\hbox{bias}}
\def\Siuu{\boldSigma_{i,uu}}
\def\whT{\widehat{\Theta}}
\def\var{\hbox{var}}
\def\cov{\hbox{cov}}
\def\corr{\hbox{corr}}
\def\sign{\hbox{sign}}
\def\trace{\hbox{trace}}
\def\naive{\hbox{naive}}
\def\vect{\hbox{vec}}


\def\Beta{\hbox{Beta}}
\def\DE{\hbox{DE}}
\def\Dir{\hbox{Dirch}}
\def\Exp{\hbox{Exp}}
\def\gIGs{\hbox{g-Inv-Gs}}
\def\Ga{\hbox{Ga}}
\def\IGs{\hbox{Inv-Gs}}
\def\IG{\hbox{Inv-Ga}}
\def\IW{\hbox{IW}}
\def\MVN{\hbox{MVN}}
\def\MatMVN{\hbox{Mat-MVN}}
\def\MVL{\hbox{MVL}}
\def\MVT{\hbox{MVT}}
\def\Normal{\hbox{Normal}}
\def\TN{\hbox{TN}}
\def\Unif{\hbox{Unif}}
\def\Mult{\hbox{Mult}}
\def\Wish{\hbox{W}}


\def\wt{\widetilde}
\def\sumi{\sum_{i=1}^n}
\def\diag{\hbox{diag}}
\def\wh{\widehat}
\def\AIC{\hbox{AIC}}
\def\BIC{\hbox{BIC}}
\def\diag{\hbox{diag}}
\def\log{\hbox{log}}
\def\bias{\hbox{bias}}
\def\Siuu{\boldSigma_{i,uu}}
\def\dfrac#1#2{{\displaystyle{#1\over#2}}}
\def\VS{{\vskip 3mm\noindent}}
\def\refhg{\hangindent=20pt\hangafter=1}
\def\refmark{\par\vskip 2mm\noindent\refhg}
\def\naive{\hbox{naive}}
\def\itemitem{\par\indent \hangindent2\parindent \textindent}
\def\var{\hbox{var}}
\def\cov{\hbox{cov}}
\def\corr{\hbox{corr}}
\def\trace{\hbox{trace}}
\def\refhg{\hangindent=20pt\hangafter=1}
\def\refmark{\par\vskip 2mm\noindent\refhg}
\def\Normal{\hbox{Normal}}
\def\Poisson{\hbox{Poisson}}
\def\Wishart{\hbox{Wishart}}
\def\Invwish{\hbox{Inv-Wishart}}
\def\Beta{\hbox{Beta}}
\def\NiG{\hbox{NiG}}
\def\matF{\hbox{Mat-F}}



\def\ANNALS{{\it Annals of Statistics}}
\def\ANNALSP{{\it Annals of Probability}}
\def\ANNALSMS{{\it Annals of Mathematical Statistics}}
\def\ANNALSAS{{\it Annals of Applied Statistics}}
\def\ANNALSISM{{\it Annals of the Institute of Statistical Mathematics}}
\def\AJE{{\it American Journal of Epidemiology}}
\def\ANIPS{{\it Advances in Neural Information Processing Systems}}
\def\APLS{{\it Applied Statistics}}
\def\BA{{\it Bayesian Analysis}}
\def\BRNL{{\it Bernoulli}}
\def\BIOK{{\it Biometrika}}
\def\BIOS{{\it Biostatistics}}
\def\BMCS{{\it Biometrics}}
\def\BMCMIDM{{\it BMC Medical Informatics and Decision Making}}
\def\BIOINF{{\it Bioinformatics}}
\def\CANADAJS{{\it Canadian Journal of Statistics}}
\def\CG{{\it Current Genomics}}
\def\CDA{{\it Computational Statistics \& Data Analysis}}
\def\COMMS{{\it Communications in Statistics, Series A}}
\def\COMMS{{\it Communications in Statistics, Theory \& Methods}}
\def\COMMSS{{\it Communications in Statistics - Simulation}}
\def\COMMSSC{{\it Communications in Statistics - Simulation and Computation}}
\def\EJS{{\it Electronic Journal of Statistics}}
\def\ECMK{{\it Econometrica}}
\def\ECTH{{\it Econometric Theory}}
\def\GENEP{{\it Genetic Epidemiology}}
\def\JASA{{\it Journal of the American Statistical Association}}
\def\JRSSB{{\it Journal of the Royal Statistical Society, Series B}}
\def\JRSSC{{\it Journal of the Royal Statistical Society, Series C}}
\def\JQT{{\it Journal of Quality Technology}}
\def\JCGS{{\it Journal of Computational and Graphical Statistics}}
\def\JCB{{\it Journal of Computational Biology}}
\def\JAMA{{\it Journal of the American Medical Association}}
\def\JNUTR{{\it Journal of Nutrition}}
\def\JABES{{\it Journal of Agricultural, Biological and Environmental Statistics}}
\def\JBES{{\it Journal of Business and Economic Statistics}}
\def\JSPI{{\it Journal of Statistical Planning \& Inference}}
\def\JMA{{\it Journal of Multivariate Analysis}}
\def\JNS{{\it Journal of Nonparametric Statistics}}
\def\JSS{{\it Journal of Statistical Software}}
\def\JECM{{\it Journal of Econometrics}}
\def\IEEE{{\it IEEE}}
\def\IEEESPL{{\it IEEE Signal Processing Letters}}
\def\IEEETIT{{\it IEEE Transactions on Information Theory}}
\def\LETTERS{{\it Letters in Probability and Statistics}}
\def\ML{{\it Machine Learning}}
\def\P_25_ICML{{\it Proceedings of the 25th international conference on Machine learning}}
\def\PLoSCB{{\it PloS Computational Biology}}
\def\STIM{{\it Statistics in Medicine}}
\def\SCAN{{\it Scandinavian Journal of Statistics}}
\def\SMMR{{\it Statistical Methods in Medical Research}}
\def\SNKH{{\it Sankhy\={a}: The Indian Journal of Statistics}}
\def\STIM{{\it Statistics in Medicine}}
\def\STATMED{{\it Statistics in Medicine}}
\def\STATSCI{{\it Statistical Science}}
\def\SSNC{{\it Statistica Sinica}}
\def\SaC{{\it Statistics and Computing}}
\def\STATSCI{{\it Statistical Science}}
\def\TECH{{\it Technometrics}}


\def\dfrac#1#2{{\displaystyle{#1\over#2}}}
\def\VS{{\vskip 3mm\noindent}}
\def\refhg{\hangindent=20pt\hangafter=1}
\def\refmark{\par\vskip 2mm\noindent\refhg}
\def\itemitem{\par\indent \hangindent2\parindent \textindent}
\def\refhg{\hangindent=20pt\hangafter=1}
\def\refmark{\par\vskip 2mm\noindent\refhg}
\def\povr{\buildrel p\over\longrightarrow}
\def\ccdot{{\bullet}}
\def\bse{\begin{eqnarray*}}
	\def\ese{\end{eqnarray*}}
\def\be{\begin{eqnarray}}
\def\ee{\end{eqnarray}}
\def\bq{\begin{equation}}
\def\eq{\end{equation}}
\def\pr{\hbox{pr}}
\def\wh{\widehat}


\def\boldalpha{{\mbox{\boldmath $\alpha$}}}
\def\boldAlpha{{\mbox{\boldmath $\Alpha$}}}
\def\boldbeta{{\mbox{\boldmath $\beta$}}}
\def\boldBeta{{\mbox{\boldmath $\beta$}}}
\def\bolddelta{{\mbox{\boldmath $\delta$}}}
\def\boldDelta{{\mbox{\boldmath $\Delta$}}}
\def\boldeta{{\mbox{\boldmath $\eta$}}}
\def\boldEta{{\mbox{\boldmath $\Eta$}}}
\def\boldgamma{{\mbox{\boldmath $\gamma$}}}
\def\boldGamma{{\mbox{\boldmath $\Gamma$}}}
\def\boldlambda{{\mbox{\boldmath $\lambda$}}}
\def\boldLambda{{\mbox{\boldmath $\Lambda$}}}
\def\boldmu{{\mbox{\boldmath $\mu$}}}
\def\boldMu{{\mbox{\boldmath $\Mu$}}}
\def\boldnu{{\mbox{\boldmath $\nu$}}}
\def\boldNu{{\mbox{\boldmath $\Nu$}}}
\def\boldomega{{\mbox{\boldmath $\omega$}}}
\def\boldOmega{{\mbox{\boldmath $\Omega$}}}
\def\boldpsi{{\mbox{\boldmath $\psi$}}}
\def\boldPsi{{\mbox{\boldmath $\Psi$}}}
\def\boldsigma{{\mbox{\boldmath $\sigma$}}}
\def\boldSigma{{\mbox{\boldmath $\Sigma$}}}
\def\boldpi{{\mbox{\boldmath $\pi$}}}
\def\boldPi{{\mbox{\boldmath $\Pi$}}}
\def\boldphi{{\mbox{\boldmath $\phi$}}}
\def\boldepsilon{{\mbox{\boldmath $\epsilon$}}}
\def\boldtheta{{\mbox{\boldmath $\theta$}}}
\def\boldTheta{{\mbox{\boldmath $\Theta$}}}
\def\boldve{{\mbox{\boldmath $\ve$}}}
\def\boldVe{{\mbox{\boldmath $\Epsilon$}}}
\def\boldxi{{\mbox{\boldmath $\xi$}}}
\def\boldXi{{\mbox{\boldmath $\Omega$}}}
\def\boldzeta{{\mbox{\boldmath $\zeta$}}}
\def\boldZeta{{\mbox{\boldmath $\Zeta$}}}
\def\boldvarrho{{\mbox{\boldmath $\varrho$}}}
\def\boldVarrho{{\mbox{\boldmath $\Varrho$}}}
\def\boldtau{{\mbox{\boldmath $\tau$}}}
\def\boldTau{{\mbox{\boldmath $\Tau$}}}
\def\boldrho{{\mbox{\boldmath $\rho$}}}
\def\boldRho{{\mbox{\boldmath $\Rho$}}}
\def\boldvarsigma{{\mbox{\boldmath $\varsigma$}}}

\def\trans{^{\rm T}}
\def\myalpha{{\cal A}}
\def\th{^{th}}
\def\bone{{\mathbf 1}}

\def\b1e{{\mathbf e}}
\def\bA{{\mathbf A}}
\def\ba{{\mathbf a}}
\def\bB{{\mathbf B}}
\def\bb{{\mathbf b}}
\def\bc{{\mathbf c}}
\def\bC{{\mathbf C}}
\def\bd{{\mathbf d}}
\def\bD{{\mathbf D}}
\def\bG{{\mathbf G}}
\def\bI{{\mathbf I}}
\def\bk{{\mathbf k}}
\def\bK{{\mathbf K}}
\def\bM{{\mathbf M}}
\def\bp{{\mathbf p}}
\def\bP{{\mathbf P}}
\def\bs{{\mathbf s}}
\def\bS{{\mathbf S}}
\def\bT{{\mathbf T}}
\def\bt{{\mathbf t}}
\def\bu{{\mathbf u}}
\def\bU{{\mathbf U}}
\def\bq{{\mathbf q}}
\def\bQ{{\mathbf Q}}
\def\bV{{\mathbf V}}
\def\bw{{\mathbf w}}
\def\bW{{\mathbf W}}
\def\bx{{\mathbf x}}
\def\bX{{\mathbf X}}
\def\by{{\mathbf y}}
\def\bY{{\mathbf Y}}
\def\bz{{\mathbf z}}
\def\bZ{{\mathbf Z}}
\def\bS{{\mathbf S}}
\def\bzero{{\mathbf 0}}

\def\whT{\widehat{\Theta}}
\def\te{\widetilde{e}}
\def\te{\widetilde{\epsilon}}
\def\tp{\widetilde{p}}
\def\tv{\widetilde{v}}
\def\tmu{\widetilde{\mu}}
\def\tsigma{\widetilde{\sigma}}

\newcommand{\etam}{\mbox{\boldmath $\eta$}}
\newcommand{\bmu}{\mbox{\boldmath $\mu$}}
\newcommand{\bDelta}{\mbox{\boldmath $\Delta$}}
\newcommand{\bphi}{\mbox{\boldmath $\phi$}}
\newcommand{\bpi}{\mbox{\boldmath $\pi$}}
\newcommand{\bPi}{\mbox{\boldmath $\Pi$}}
\newcommand{\bxi}{\mbox{\boldmath $\xi$}}
\newcommand{\bepsilon}{\mbox{\boldmath $\epsilon$}}
\newcommand{\btheta}{\mbox{\boldmath $\theta$}}
\newcommand{\bbeta}{\mbox{\boldmath $\beta$}}
\newcommand{\bgamma}{\mbox{\boldmath $\gamma_{j}$}}
\newcommand{\bzeta}{\mbox{\boldmath $\zeta$}}
\newcommand{\bsigma}{\mbox{\boldmath $\sigma$}}
\newcommand{\bSigma}{\mbox{\boldmath $\Sigma$}}
\newcommand{\balpha}{\mbox{\boldmath $\alpha$}}
\newcommand{\bomega}{\mbox{\boldmath $\omega$}}
\newcommand{\blambda}{\mbox{\boldmath $\lambda$}}
\newcommand{\bLambda}{\mbox{\boldmath $\Lambda$}}
\newcommand{\bOmega}{\mbox{\boldmath $\Omega$}}
\newcommand{\bPsi}{\mbox{\boldmath $\Psi$}}
\newcommand{\bpsi}{\mbox{\boldmath $\psi$}}
\newcommand{\bGamma}{\mbox{\boldmath $\Gamma$}}
\newcommand{\btau}{\mbox{\boldmath $\tau$}}

\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\norm}[1]{\left\Vert#1\right\Vert}

\newcommand{\uA}       {\mbox{\boldmath$A$}}
\newcommand{\ua}       {\mbox{\boldmath$a$}}
\newcommand{\uB}       {\mbox{\boldmath$B$}}
\newcommand{\ub}       {\mbox{\boldmath$b$}}
\newcommand{\uC}       {\mbox{\boldmath$C$}}
\newcommand{\uc}       {\mbox{\boldmath$c$}}
\newcommand{\uD}       {\mbox{\boldmath$D$}}
\newcommand{\ud}       {\mbox{\boldmath$d$}}
\newcommand{\uE}       {\mbox{\boldmath$E$}}
\newcommand{\ue}       {\mbox{\boldmath$e$}}
\newcommand{\uF}       {\mbox{\boldmath$F$}}
\newcommand{\uf}       {\mbox{\boldmath$f$}}
\newcommand{\uG}       {\mbox{\boldmath$G$}}
\newcommand{\ug}       {\mbox{\boldmath$g$}}

%\newcommand{\uG}       {\mbox{\boldmath$G$}}

%\newcommand{\ug}       {\mbox{\boldmath$g$}}
\newcommand{\uH}       {\mbox{\boldmath$H$}}
\newcommand{\uh}       {\mbox{\boldmath$h$}}
\newcommand{\uI}       {\mbox{\boldmath$I$}}
\newcommand{\ui}       {\mbox{\boldmath$i$}}
\newcommand{\uJ}       {\mbox{\boldmath$J$}}
\newcommand{\uj}       {\mbox{\boldmath$j$}}
\newcommand{\uK}       {\mbox{\boldmath$K$}}
\newcommand{\uk}       {\mbox{\boldmath$k$}}
\newcommand{\uL}       {\mbox{\boldmath$L$}}
\newcommand{\ul}       {\mbox{\boldmath$l$}}
\newcommand{\uM}       {\mbox{\boldmath$M$}}
\newcommand{\um}       {\mbox{\boldmath$m$}}
\newcommand{\uN}       {\mbox{\boldmath$N$}}
\newcommand{\un}       {\mbox{\boldmath$n$}}
\newcommand{\uO}       {\mbox{\boldmath$O$}}
%\newcommand{\uo}       {\mbox{\boldmath$o$}}
\newcommand{\uP}       {\mbox{\boldmath$P$}}
\newcommand{\up}       {\mbox{\boldmath$p$}}
\newcommand{\uQ}       {\mbox{\boldmath$Q$}}
\newcommand{\uq}       {\mbox{\boldmath$q$}}
\newcommand{\uR}       {\mbox{\boldmath$R$}}
\newcommand{\ur}       {\mbox{\boldmath$r$}}
\newcommand{\uS}       {\mbox{\boldmath$S$}}
\newcommand{\us}       {\mbox{\boldmath$s$}}
\newcommand{\uT}       {\mbox{\boldmath$T$}}
\newcommand{\ut}       {\mbox{\boldmath$t$}}
\newcommand{\uU}       {\mbox{\boldmath$U$}}
\newcommand{\uu}       {\mbox{\boldmath$u$}}
\newcommand{\uV}       {\mbox{\boldmath$V$}}
\newcommand{\uv}       {\mbox{\boldmath$v$}}
\newcommand{\uW}       {\mbox{\boldmath$W$}}
\newcommand{\uw}       {\mbox{\boldmath$w$}}
\newcommand{\uX}       {\mbox{\boldmath$X$}}
\newcommand{\ux}       {\mbox{\boldmath$x$}}
\newcommand{\uY}       {\mbox{\boldmath$Y$}}
\newcommand{\uy}       {\mbox{\boldmath$y$}}
\newcommand{\uZ}       {\mbox{\boldmath$Z$}}
\newcommand{\uz}       {\mbox{\boldmath$z$}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\ualpha}            {\mbox{\boldmath$\alpha$}}
\newcommand{\ubeta}             {\mbox{\boldmath$\beta$}}
\newcommand{\ugamma}            {\mbox{\boldmath$\gamma$}}
\newcommand{\udelta}            {\mbox{\boldmath$\delta$}}
\newcommand{\uepsilon}          {\mbox{\boldmath$\epsilon$}}
\newcommand{\uvarepsilon}       {\mbox{\boldmath$\varepsilon$}}
\newcommand{\uzeta}             {\mbox{\boldmath$\zeta$}}
\newcommand{\ueta}              {\mbox{\boldmath$\eta$}}
\newcommand{\utheta}            {\mbox{\boldmath$\theta$}}
\newcommand{\uvartheta}         {\mbox{\boldmath$\vartheta$}}
\newcommand{\uiota}             {\mbox{\boldmath$\uiota$}}
\newcommand{\ukappa}            {\mbox{\boldmath$\kappa$}}
\newcommand{\ulambda}           {\mbox{\boldmath$\lambda$}}
\newcommand{\umu}               {\mbox{\boldmath$\mu$}}
\newcommand{\unu}               {\mbox{\boldmath$\nu$}}
\newcommand{\uxi}               {\mbox{\boldmath$\xi$}}
\newcommand{\uo}                {\mbox{\boldmath$\o$}}
\newcommand{\upi}               {\mbox{\boldmath$\pi$}}
\newcommand{\uvarpi}            {\mbox{\boldmath$\varpi$}}
\newcommand{\urho}              {\mbox{\boldmath$\rho$}}
\newcommand{\uvarrho}           {\mbox{\boldmath$\varrho$}}
\newcommand{\usigma}            {\mbox{\boldmath$\sigma$}}
\newcommand{\uvarsigma}         {\mbox{\boldmath$\varsigma$}}
\newcommand{\utau}              {\mbox{\boldmath$\tau$}}
\newcommand{\uupsilon}          {\mbox{\boldmath$\upsilon$}}
\newcommand{\uphi}              {\mbox{\boldmath$\phi$}}
\newcommand{\uvarphi}           {\mbox{\boldmath$\varphi$}}
\newcommand{\uchi}              {\mbox{\boldmath$\chi$}}
\newcommand{\upsi}              {\mbox{\boldmath$\psi$}}
\newcommand{\uomega}            {\mbox{\boldmath$\omega$}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\uGamma}            {\mbox{\boldmath$\Gamma$}}
\newcommand{\uDelta}            {\mbox{\boldmath$\Delta$}}
\newcommand{\uTheta}            {\mbox{\boldmath$\Theta$}}
\newcommand{\uLambda}           {\mbox{\boldmath$\Lambda$}}
\newcommand{\uXi}               {\mbox{\boldmath$\Xi$}}
\newcommand{\uPi}                {\mbox{\boldmath$\Pi$}}
\newcommand{\uSigma}            {\mbox{\boldmath$\Sigma$}}
\newcommand{\uUpsilon}          {\mbox{\boldmath$\Upsilon$}}
\newcommand{\uPhi}              {\mbox{\boldmath$\Phi$}}
\newcommand{\uPsi}              {\mbox{\boldmath$\Psi$}}
\newcommand{\uOmega}            {\mbox{\boldmath$\Omega$}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\myx{T}
\def\curr{_{\rm curr}}
\def\Dobs{{\bf \cal D}_{\rm obs}}
\def\Didobs{{\bf \cal D}_{id,\rm obs}}
\def\Supp{{\bf Supplementary Material}}
\DeclareMathOperator*{\argmin}{arg\,min}

\newtheorem{mydef}{Definition}
% Title Page
\title{An approximate Bayes factor based high dimensional MANOVA using Random Projections}
\author{Roger S. Zoh}
%\bibliographystyle{plain}

\begin{document}
	\maketitle

\begin{abstract}
High-dimensional mean vector testing problem for two or more groups remain a very active research area. In these setting, traditional tests are not applicable because they involve the inversion of rank deficient group covariance matrix. In current approaches, this problem is addressed by simply looking at a test assuming a sparse or diagonal covariance matrix potentially ignoring complex dependency between features.
In this paper, we develop a Bayes factor (BF) based testing procedure for comparing two or more population means in (very) high dimensional settings. Two versions of the Bayes factor based test statistics are considered which are based on a Random projection (RP) approach. RPs are appealing since they make not assumption about the form of the dependency across features in the data. The final test statistic is based on an ensemble of Bayes factors corresponding to multiple replications of randomly projected data. Both proposed test statistics are compared through a battery of simulation settings. Finally they are applied to the analysis of a publicly available genomic single cell RNA-seq (scRNA-seq) dataset. 

%In ‘large-p-small-n’ settings, Bayes factors based on proper priors require eliciting a large and potentially complex $p\times p$ covariance matrix. Bayes factors based on Jeffrey’s prior suffer the same impediment as the other classical test statistics as they involve inversion of ill-formed sample covariance matrices. To circumvent this limitation, we propose that the Bayes factor be based on lower dimensional random projections of the high dimensional data vectors. The final test statistic is based on an ensemble of Bayes factors corresponding to multiple replications of randomly projected data. We consider two potential test statistics and show that the test has reasonable properties. We demonstrate the efficacy of the approach through simulated and real data examples.
\end{abstract}

\section{Introduction} \label{sec:intro}
The problem of comparing multiple group means continue to receive considerable attentions in literature, especially in the ‘large-p-small-n’ setting where $p >> n$. For the two (or sample) samples testing problem, the approaches proposed in literature all center around a version of the Hotelling's $T^2$ statistic. Namely the statistic used is
\be
T^{2} = C_{n}(\overline{\uX} - \overline{\uY})\trans\uS^{-1} (\overline{\uX} - \overline{\uY}), \label{eq:eqT2}
\ee
where $C_{n}$ is free a data free quantity; $\uS$ is the (pooled) sample covariance; the sample mean vectors are $\bar{\uY}$ and $\bar{\uX}$.
Unfortunately, in its original (\ref{eq:eqT2}), the $T^2$ statistic can be quickly become ill formed since it involves the inversion of a sample covariance matrix that is not positive definite when the dimension of the vector exceeds or supersedes the combined sample sizes. Various approaches exits in literature to help circumvent these limitations. Solutions to that problem have centered around $4$ main approaches. One approach assume a specific structure, no or specific dependency structure between the features or groups of features. This has the direct effect of removing the issue of inverting ill-formed covariance matrices. References of these approaches can be found in (\citealp{bai1996effect}, \citealp{chen2010two}, \citealp{ahmad2014u}, \citealp{feng2017composite}) among others. A second approach can be viewed as a regularization scheme with a goal of making the sample covariance invertible. Two regularization schemes have emerged (\citealp{hu2020pairwise}). One regularization scheme uses a ridge type estimator for the sample covariance matrix (see \citealp{chen2011regularized}, \citealp{li2020adaptable}). Another regularization approach, which is in principle closer to a data perturbation approach than a regularization approach as we commonly know it, is based on a random projection approach. This approach works by projecting the originally high dimensional data to a low-dimensional embedding and perform the test in this lower dimension data, completely eliminating the need to inverse a rank degenerate sample covariance matrix. Reference of paper using this approach are \cite{lopes2011more}; \cite{thulin2014high}; \cite{srivastava2014raptt} in the frequentist setting and \cite{zoh2018powerful} in the Bayesian setting. Recently, there is a growing effort towards combining these two approach in the two-group mean testing problem (\citealp{hu2020pairwise}).

%As seen before the two group mean testing problem continue to be an active research area.
Two-sample mean testing problem in high dimension setting is a special case of the more general MANOVA (Multivariate Analysis of Variance) problem. Suppose $K$ populations of dimension $p$, mean vector specified respectively as $\umu_1, \cdots, \umu_{K}$, and common covariance matrix $\uSigma$. In the MANOVA testing problem is formulated as:
\be
H_{0}:\; \umu_i = \umu_j\; \; \forall \; (i,j) \in \mathcal{P}\;  \; \mbox{versus} \; \; H_{1}:\; \exists \; (i,j) \in \mathcal{P}\; \mbox{s.t}   \; \umu_i \neq \umu_j\; \in \mathcal{P}  \label{eq:test1}
\ee
where $\mathcal{P} = \{(i,j): 1 \leq i < j \leq p  \}$.
Early references on the more general (more than two groups) MANOVA approach when $p$ exceeds the sample sizes in literature can be found in the work of \cite{dempster1958high}; \cite{dempster1960significance}. In general, the approaches presented in literature center around two major assumptions. One approach derives the test under the assumption of common variances across groups (\cite{fujikoshi2004asymptotic}). Another approach removes the assumption of common covariances (\citealp{srivastava2007multivariate}).

 Recently, random matrix approaches in general and random projections (RP) in particular have emerged as effective (linear) data reduction techniques which have been used in many fields. See \citealp{wan2020sharp}; \citealp{lopez2021tuning} to list just a few. Additionally, RP have already been proven very successful in the two-sample group tests \cite{lopes2011more, srivastava2014raptt,zoh2018powerful}. However, to the best of our knowledge, they have not been used or evaluated in the multiple (MANOVA) groups mean testing problem. The goal of this paper is to investigate the performance of RPs in the Bayes factor based test in the MANOVA setting. The paper is structure as follows. In section~\ref{sec:test}, we derive the Bayes factor based tests; section~\ref{sec:simul} evaluate empirical properties of the proposed test through a battery of simulation; in section~\ref{sec:Application} we apply the proposed method to the analysis of an actual data set. We end with some concluding remarks in section~\ref{sec:conclusion} and provide theoretical results of our test in section~\ref{sec:theori}.

\section{Bayes factor based test} \label{sec:test}
Here, we describe our approach. Suppose the following data generating model. $\uX_{ig} = \umu_i + \uepsilon_{ig}$, $g = 1, \cdots, G$.
 Note here that we assume that $G \geq 2$ and $\uepsilon_{ig} \stackrel{iid}{\sim} \MVN_{p}(\bzero, \uSigma)$; $\MVN_{p}$ denotes a multivariate-normal distribution with dimension $p$. Suppose the data matrices are observed (independently) for each of $G> 1$ groups as $\uX_1 \in \mathbb{R}^{n_1 \times p}, \cdots,  \uX_G \in \mathbb{R}^{n_G \times p}$. 
Let's denote $\mathcal{P} = \{(i,j): 1 \leq i < j \leq G \}$ the collection all unique group indices. %A set of sufficient statistics for the data is $\left(\bar{\uX}_1, \bar{\uX}_2, \cdots, \bar{\uX}_G, \uS_{p}\right)$. Note that $\uS_{p}$ is the pooled covariance estimate of $\uSigma$.
Let's $\udelta_{ij} = \umu_i - \umu_j$, the compound hypothesis in $(~\ref{eq:test1})$ is expressed as
\be
H_0:\; \udelta_{ij} &=& \uzero\;\;\forall (i,j) \in \mathcal{P} \nonumber \\
 &\mbox{vs}& \nonumber \\
 H_1: \; \exists (i,j) \in \mathcal{P}\;\; \mbox{s.t}\;\; \udelta_{ij} &\sim& \MVN_{p}(\uzero, \uSigma_{ij}/\tau_{0,ij})\; \text{and}\; \uSigma_{ij} \propto |\uSigma_{ij}|^{-(p+1)/2} \; \label{eq:hyp1}
\ee
which is equivalent to performing $G(G-1)/2$ pairwise comparisons similar to the approach taken in \cite{tony2014two} and \cite{ahmad2014u}. Although the tests proposed in literature almost all adopt a component wise (vector entry) test, here we approach this testing problem while preserving (or minimally disturbing) the dependencies between the vector coordinates. To that end our test uses the Bayes Factor approach and we discuss in detail the two tests we consider.

\subsection{Bayes factor Based on Pooled covariance matrix ($BF^{p}$)} \label{sec:testpl}
For the case when $G = 2$ in high-dimensional settings $p >> n_1+n_2$, \cite{zoh2018powerful} proposed a Bayes Factor  based test using the idea of Random projection (RP). Namely, in a two-group setting, the $BF$ based test using RP matrix $\uPhi$ is defined as
\be
BF_{10}(\uR) &=& \left(1 + \eta_{12} \right)^{-m/2} \left\{\frac{  1 + \frac{mf}{(1 + \eta_{12})(N-m-(G-1))}}{ 1 + \frac{p f}{(N-m-(G-1))}  } \right\}^{-(N-1)/2}, \label{eq:BF1}
\ee
where $\tau_0$ is the scale parameter for the prior under the alternative; $\eta_{12} = n_{0,12}/\tau_{0}$ and $f_{1,2}  = \frac{N-m-1)}{(N-2)m} n_{0,12} (\overline{\uX}_1 - \overline{\uX}_2)\trans\Phi(\Phi\trans\uS_{p}\Phi\trans)^{-1}\Phi\trans(\overline{\uX}_1 - \overline{\uX}_2)$; $N = n_1 + n_2$;
%$f_{1,2}  = \frac{N-m-1)}{(N-2)m} n_{0,12} (\overline{\uX}_1 - \overline{\uX}_2)\uR\trans (\uR\trans\uS_{p}\uR)^{-1}\uR\trans(\overline{\uX}_1 - \overline{\uX}_2)$; $N = n_1 + n_2$;
 $ 1/n_{0,12} = 1/n_1 + 1/n_2$; $\overline{\uX}_g$ is group $g$ sample mean; $\uS_p$ is the $p \times p$ sample covariance matrix; $\Phi \in \mathbb{R}^{p \times m}$ is the projection matrix and $m$ is the lower dimensional projection space chosen so that $m < n_1 +n_2 - 2 << p$. \cite{zoh2018powerful} also proposed an automatic approach to select $m$, $\tau_0$. Note here that $m$ depends on $n_1$ and $n_2$ but is totally independent of $p$.
 Similarly,  to test the (complex) hypothesis in (\ref{eq:hyp1}), we can use the following test statistic:
\be
BF^{p}_{10} &=& \left(1 + \eta_{ij} \right)^{-p/2} \left\{ \frac{  1 + \frac{pf^{max}}{(1 + \eta_{ij})(N-p-(G-1))}}{ 1 + \frac{p f^{max}}{(N-p-(G-1))}  } \right\}^{-(N-1)/2}, \label{eq:BFmax}
\ee
where $f^{max} = \underset{(i,j)}{\mathrm{argmax}}  f_{ij}$ is the maximum over all the pairwise $f_{ij}$ (data dependent) statistics; $N = \sum^{G}_{g=1}n_g$. Equivalently, $\eta_{ij} = n_{0, ij}/\tau_{0,ij}$ and is based on the pair that yields the highest $\uF$ ($f_{ij}$) statistic. Note that $\uS_p$ is the pooled (all the groups) sample covariances. It is important to note that under the data model, we have that $\forall\;\;(i,j) \in \mathcal{P},  f_{ij} \stackrel{id}{\sim} \uF_{m, N - m- (G-1)}$ (identically but not independently distributed) when the NULL hypothesis is true. The lack of independence renders the derivation of the NULL distribution (or quantiles of the NULL distribution) of $f^{max}$ difficult. We defer the discussion about the choice of $m$ and $\tau_0$ to later. We show the plot comparing the simulated NULL distribution of $f^{max}$ statistics for various group size and equal group sample sizes for various (common) covariance matrices. (We need to insert the plot of NULL distribution for $f_{1,2}$).

%When $p >> \sum^{G}_{g=1}n_g - G$, our test statistics is not well defined since the pooled sample covariance is (nearly) singular and not invertible. To address that issue, current approaches center around a two-stage approach where the covariance (precision) matrix is first estimated using some regulation steps (see \cite{cai2014high}) and or by considering eliminating the need to compute inverse (see \cite{ahmad2019multiple}).
%
%We take a completely different approach using a random projections where we first project the data in a lower dimension $m < \min{ \sum^{G}_{g=1}n_g - G, p}$. Namely, for random projection matrix $\uPhi \in \mathcal{R}^{p \times m}$, we compute the data dependent part of our test statistics defined in Equation~\ref{eq:BFmax} as:
%\be
%f_{i,j}  &=& \frac{N-m-(G-1)}{(N-G)m} n_{0,ij} (\overline{\uX}_i - \overline{\uX}_j)\uR\trans (\uR\trans\uS_{p}\uR)^{-1}\uR\trans(\overline{\uX}_i - \overline{\uX}_j);   \label{eq:fpull}.
%\ee
%We delay the discussion on the choices of $m$ and the entries of the projection matrix $\uPhi$ to later section.
%Note that (\ref{eq:BFmax}) is a special case of (\ref{eq:BF1}). Before hand, we need to specify the value of $\tau_0$. We will adopt the approach of \cite{zoh2018powerful} to estimate $\tau_0$ or each pairs.
%Properties of the test proposed in (\ref{eq:BFmax}) are similar to the what was proposed in \cite{zoh2018powerful}.

\subsection{Bayes factor Based on paired covariance matrix ($BF^{I}$)} \label{sec:testid}
The Bayes factor proposed in Equation \ref{eq:BFmax} relies on a pooled single covariance matrix $\uS_p$ implying similar covariance matrices across all groups. The assumption of common covariance matrix across groups can reveal very powerful as it allows to borrow information across for amore precise estimate of the common covariance matrix $\uSigma$. However, it can also be detrimental if grossly wrong. We relax that assumption by instead using a pooled pairwise covariance matrices, which is a less stringent assumption than an assumption of covariance matrices. We then get
\be
BF^{I}_{10} &=& \left(1 + \eta_{ij} \right)^{-m/2} \left\{ \frac{  1 + \frac{m f^{max}}{(1 + \eta_{ij})(N_{ij}-m-1)}}{ 1 + \frac{m f^{max}}{(N_{ij}-m-1)}  } \right\}^{-(N_{ij}-1)/2}, \label{eq:BFmaxij}
\ee
where $f^{max} = \underset{(i,j)}{\mathrm{argmax}}\;\;f_{ij}$;
When $m < \underset{(i,j)}{\mathrm{argmin}} N_{ij} - 2 << p$, again $f_{i,j}  = \frac{(N_{ij}-m-1)}{(N_{ij}-2)m} n_{0,ij} (\overline{\uX}_i - \overline{\uX}_j)\trans\Phi (\Phi\trans\uS_{p,ij}\Phi)^{-1}\Phi\trans(\overline{\uX}_i - \overline{\uX}_j)$;
$N_{ij} = n_i +n_j$; $\uS_{p,ij}$ is the sample covariance for both group $i$ and $j$. Note that $\forall (i,j) \in \mathcal{P}, \; f_{ij} \stackrel{id}{\sim} \uF_{p, N_{ij} - m- 1}$ (identically but not independently distributed). Again, similarly to the observation made above, here also the lack of independence renders the derivation of the NULL distribution (or quantiles of the NULL distribution) of $f^{max}$ difficult. We show the plot comparing the simulated NULL distribution of $f^{max}$ statistics for various group size and equal group sample sizes.

%However if $p > \underset{(i,j)}{\mathrm{argmin}} \sum_{g} N_{ij} - 2$, then we first project the data in a lower dimension $m < \min{p, N_{i,j}-2}$. Namely, for a RP matrix $\uPhi \in \mathcal{R}^{p \times m}$ where $\uPhi\trand\uPhi = \uI_{m}$, we get:
%$f_{i,j}  = \frac{(N_{ij}-m-1)}{(N_{ij}-2)m} n_{0,ij} (\overline{\uX}_i - \overline{\uX}_j)\uR\trans (\uR\trans\uS_{p,ij}\uR)^{-1}\uR\trans(\overline{\uX}_i - \overline{\uX}_j)$;
%$N_{ij} = n_i +n_j$; $\uS_{p,ij}$; $\forall 1<i<j<G;\; f_{ij} \stackrel{id}{\sim} \uF_{m, N_{ij} - m- 1}$ (identically but not independently distributed). 
%Again, similarly to the observation made above, here also the lack of independence renders the derivation of the NULL distribution (or quantiles of the NULL distribution) of $f^{max}$ difficult. We show the plot comparing the simulated NULL distribution of $f^{max}$ statistics for various group size and equal group sample sizes.

%When $p >> N = \sum^{G}_{g=1} n_g - G$, then (\ref{eq:BFmax}) is ill-posed and can't be computed since $\uS_p$ is no-longer invertible. We can address that issue redefining the testing problem by looking at the testing in term of specific features similar to the approach taken by (ReF Lopes - MANOVA test and Cai). Instead, we adopt a dimension reduction approach thereby (almost) preserving all the potentially complex dependency among the p features. Our approach proceeds as follows, for a specific choice of a random projection(RP) matrix $\Phi$, the BF in (\ref{eq:BFmax}) becomes
%\be
%BF^{max}_{10}(\Phi) &=& \left(1 + \eta_{ij} \right)^{-p/2} \left\{ \frac{  1 + \frac{pf^{max}(\Phi)}{(1 + \eta_{ij})(N-p-(G-1))}}{ 1 + \frac{p f^{max}(\Phi)}{(N-p-(G-1))}  } \right\}^{-(N-1)/2}, \label{eq:BFmaxrp}
%\ee
%where $\Phi \in \mathcal{R}^{p \times m}$, with $m << p$ and $\Phi\trans\Phi = \uI_m$. Note that $\uI_m$ is the identity matrix with dimension $m$. With the choice of the RP matrix, we project the data from dimension $p$ into a lower dimension $m$, where $m < \min\{N, p\}$. We prescribe a mean to choose $m$ below.

\subsection{Ensemble test} \label{sec:testens}
Based on the BF statistics in (\ref{eq:BFmax}) and (\ref{eq:BFmaxij}), we will opt in favor of the alternative if the $BF$ (maximum BF) exceeds a given evidence threshold $\ugamma$  (see \citealp{kass1995bayes}). We provide a way to objectively choose $\ugamma$ later. A test based on a single RP $\uPhi$ can hugely depend on the choice of that single RP matrix. Instead, we based our final decision on multiple RPs and the final decision is reached based on an ensemble test. Hence, for $N$ chosen sufficiently large, our final test statistic is obtained as
\be
\widetilde{\boldpsi}_{N} &=& \frac{1}{N}\sum^{N}_{i=1} \bone\{ BF_{10}(\Phi_i) \geq \gamma\}; \label{eq:BFensbl}
\ee
where $\bone\{A\}$ is the indicator function which equals $1$ if $A$ is true and zero otherwise. Note here that $\gamma$ depends on $n$. The NULL distribution of the test statistics (\ref{eq:BFensbl}) is difficult to derive analytically but under our assumed data generating model, it is (cheaply) approximated. Additionally, the NULL distribution of the test statistics is invariant under an arbitrary common mean vector and common (unknown) covariance matrix $\uSigma$. Provided the NULL distribution our test statistic $\widetilde{\boldpsi}^{0}$, the decision is made as follows:
\be
 \left \{
       \begin{array}{llll}
       \mbox{Reject}~ H_{0} & \mbox{if} ~~ \widetilde{\boldpsi}(N) > \widetilde{\boldpsi}^{0}_{\alpha},  \\
       \mbox{Accept}~ H_{0} & \mbox{Otherwise}, \label{eq:eqBFfin}
       \end{array}
       \right.
\ee
Here $\widetilde{\boldpsi}^{0}_{\alpha}$ denotes the upper $\alpha$ percentile of the NULL distribution.  
We show that the test is unbiased and its power converges to $1$ with increasing sample sizes. We formalize these results in the theory section below (Section~\ref{sec:theori}).  

\subsection{Choices of $m$, $\tau_{0,ij}$ and $\ugamma$} \label{sec:testmtaugam}
We obtain values for $m$, $\tau_{0,ij}$, and $\ugamma$ using the idea of restricted most powerful Bayesian test (RMPBT) of \citealp{GoddardJohnson,Goddard}. To find the RMPBT, we would like to choose the parameters for the alternative (parameters of the distribution under the alternative) so to maximize the probability of rejecting the NULL under all possible data generating distribution. Namely, we select $\tau_{0,ij}$ so that
$$P\{BF^{max}_{10}(\tau_0) \geq  \ugamma\} \geq P\{BF^{max}_{20}(\tau_1) \geq \ugamma \},$$
for two different choices of the prior parameters under the alternative.
This is equivalent to choosing $\tau_{0,ij}$ so that  $P\{f^{max} > f^{max}_{0}(\tau_{0,ij})\}$ is maximized.
It is easy to see that we maximize that probability if $f^{max}_{0}(\tau_{0,ij})$ is minimized over all possible values of $\tau_{0,ij}$. Note that $f^{max}_{0}(\tau_{0,ij}) =\frac{1+\eta_{ij}}{\eta_{ij}} \left\{ \frac{(N - m -(G-1))C_{ij}}{m(1 - C_{ij})} \right\}$, where
$C_{ij} = \frac{1+\eta_{ij}}{\eta_{ij}}\left\{ 1 - \{\gamma(1+\eta_{ij})^{m/2} \}^{-2/(N-1)}  \right\}$ (for Bayes Factor in \ref{eq:BFmax}) and $C_{ij} = \frac{1+\eta_{ij}}{\eta_{ij}}\left\{ 1 - \{\gamma(1+\eta_{ij})^{m/2} \}^{-2/(n_i+n_j-1)}  \right\}$ and $N_{ij} = n_i +n_j$ (for Bayes Factor in \ref{eq:BFmaxij}); and $\eta_{ij} = n_{0,ij}/\tau_{0,ij}$. Note that $f^{max}_{0}(\tau_{0,ij})$ also depends on $m$, the dimension of the projection space. Noting that for each BF based test, $f_{ij}$ for all $1 \leq i < j \leq G$ have a marginal $F$ distribution but a complex joint distribution. Then we have that for a chosen value of $\alpha$ if $f^{max}_{0}(\tau_{0,ij})$ is the upper $\alpha$ percentile of the distribution of $f^{max}$, then $F_{m, N-m-(G-1)}(\alpha) \leq f^{max}_{0}(\tau_{0,ij})$, for the BF in (\ref{eq:BFmax}).  We can use that fact to obtain an approximate value of $m$ in both cases as:
\be
 \underset{m}{\mathrm{arg\min}}\; F_{m, N -m -(G-1)}(\alpha)\;\; \mbox{or}\;\; \underset{m}{\mathrm{\min}} \left\{m:\forall\;(i,j) \in\mathcal{P}\;,  \underset{m}{\argmin}\; F_{m, n_i+n_j -m -1}(\alpha) \right\}, \label{eq:mval} 
 \ee
for the BF in (\ref{eq:BFmax}) and (\ref{eq:BFmaxij}) respectively; $F_{a, b}(\theta)$ is the upper $\theta \in (0, 0.5)$ percentile of a $F$ distribution with $a$ and $b$ degrees of freedom. We discuss the choice of $\alpha$ shortly.
Next, given $n_1,n_2, \cdots, n_{G}$ and $m$, the null distribution of $f^{max}$ can simply be approximated under the assumption of common covariance matrix using a monte carlo approach. Next we obtain, $\tau_{0,ij}$ as
\be
\tau_{0,ij}(n) &=& \frac{n_{0,ij}}{f^{max,0}(\alpha) - 1}, \label{eq:tau0}
\ee
for a significance level $\alpha$;  $1/n_{0,ij} = 1/n_{i} + 1/n_{j}$. Recall $f^{max,0}(\alpha)$ denotes the upper $\alpha$ percentile of a null distribution of $f^{max}$. The choice the $f^{max,0}(\alpha)$ allows us to easily compare our proposed test with a non-Bayesian competitor using a chosen significance level $\alpha$.
 Additionally, we can choose the threshold as follows $\gamma_{ij,\alpha} = \left\{ 1 + \eta_{ij}\right\}^{-m/2}\left\{ 1 - \frac{\eta_{ij}}{1 + \eta_{ij}} C_{ij} \right\}^{-(N -1)/2}$ (for Bayes Factor in \ref{eq:BFmax}) and  %\label{eq:gamma}
$\gamma_{ij,\alpha} = \left\{ 1 + \eta_{ij}\right\}^{-m/2}\left\{ 1 - \frac{\eta_{ij}}{1 + \eta_{ij}} C_{ij} \right\}^{-(n_i+n_j -1)/2}$ (for Bayes Factor in \ref{eq:BFmaxij}).
Finally, the RP matrices are chosen orthogonal matrices so that for a given RP matrix $\Phi \in \mathbb{R}^{p \times m}$,  $\Phi\trans\Phi = \uI_{m}$. We use the sparse and dense version of the RP matrices proposed by \cite{srivastava2014raptt}. Additionally, the normalization step can be completely skipped based on the results of the QR factorization of a matrix (see \citealp{zoh2018powerful} for more details).
%Each cases of the test statistics considered, the data depending quanty is a $f^{max}$ an $\uF$ statistics.

%First note that for each pair $(i<j) \in \{1, \cdots, p\}$, $f_{i,j}  = \frac{N-m-1)}{(N-2)m} n_{0,ij} (\overline{\uX}_i - \overline{\uX}_j)\trans \uS_{p}^{-1}(\overline{\uX}_i - \overline{\uX}_j) \sim \uF_{m, N-m-(G-1)}$ (marginally). Although the $f_{ij}$ are identically distributed, they are not independent. In fact the distribution of $f_{max}$ is complicated to derive but can simply be approximated under the NULL using a Monte Carlo approach. The simulation will need to be run for multiple values of $m$ which can reveal too expensive. We adopt an approximate approach and obtain $m$ almost as if we were performing a two-sample approach  using the definition of restricted most powerful Bayesian test (RMPBT) approach proposed by \cite{GoddardJohnson,Goddard}. First note that
%Namely, we choose the value of m as
%$$ m = \arg\max_{m} \uF_{m, N-m-(G-1)}(\theta), $$
%where $uF_{\alpha,\beta}(u)$ is the upper $u$ quantile of the $\uF$ distribution with $m$ and $N-m-(G-1)$ degrees-of-freedom. Subsequently, given the value of $m$, we obtain $\gamma$ as
%$$\log(\gamma) =  -.5m\log(1+\tau^{\star}) - .5(N-1)\log\left(1 - \frac{\tau^{\star}}{1 + \tau^{\star}}C_0 \right),$$
%where $C_0 = \frac{mf^{max}_{\theta}}{m f^{max}_{\theta} + N-m-(G-1)}$; $f^{max}_{\theta}$ is the upper $\theta$ quantile of the null distribution of $f^{max}$; $\tau^{\star} = f^{max}_{\theta} -1$. Note for we simply approximate the (NULL) distribution of  $f^{max}$ using a Monte Carlo approach given values of $n_1, n_2, \cdots, n_{G}$ and $m$.

\section{Simulations $\&$ Results} \label{sec:simul}
%\subsection{Simulation Study (1)}
%Our simulation exercise follows closely resemble the simulation approach considered in \cite{tony2014two}.
%We divide our simulation exercise in $3$ parts. In the first part, we simulate data from the multivariate normal with various covariance matrices similar to the one considered in \cite{tony2014two}. We will consider $G=3$ and $G=5$ groups with $n = 60$ and $n=100$ sample size for each group and $p = 400, 1000$. We will assume $\umu_1 = \bzero$ (always), and then $\umu_j = (\mu_{jl})$ for $j \in \{2, \cdots, G\}$ and $l \in \{1, \cdots,p\}$ where all entries of $(\mu_{jl})$ are all zeros under the NULL. Under the alternative, $p_0$ entries of each group mean vector is set at zero. The non-zero entries are uniformly drawn between $[-\sqrt{2\log(p) /n}, \sqrt{2\log(p)/n}]$.

\subsection{Simulation Study design}
We designed a simulation study aimed at investigating the power of the tests proposed in section~\ref{sec:test}
with respect to the proportion of true elements of $\umu$ that are actually zero (for none all zero vectors) for various choices of covariance matrices and different proportions of non zeros vectors.
Broadly, we consider two settings for our simulation. In each case, we have two conditions for each choice of the covariance matrix. 
In the first condition, we assume $p = 200$, $G=3, 5$ and $n_g = 50$, $\forall\; 1 \leq g \leq G$. Using the approach described above (Section~\ref{sec:test}), we find $m = 43$.
In the second condition, $p = 1000$, $G=3, 5$ and $n_g = 70$, $\forall\; 1 \leq g \leq G$ and we get $m = 62$. We denote by $p_0$ the proportion of entries of the vector $\udelta$ that are exactly zero. We choose $p_0 = 0.5, .75, .80, 0.95, 0.99, 1.00$ (NULL hypothesis).
In each setting, the values of $\tau_{\alpha}$ and $\gamma_{\alpha}$ are chosen according to our discussion in Section~\ref{sec:testmtaugam}. We consider two types of random projections matrices, $\uR_{1}$(full matrix)  and $\uR_2$ (sparse matrix) as described in \citealp{srivastava2014raptt,zoh2018powerful}.

In \textbf{case 1}, only the last group $G$ has a non-zero mean vector $\umu_G$. But all the others groups have vector mean zero. In \textbf{case 2}, only the last group $G$ has a zero vector mean $\umu_{G}$ and a fixed proportions of the entries of the other group mean vectors are selected to be non-zeros.  
We consider the following choices of covariance matrix $\uSigma=(\sigma_{ij})$.
\begin{enumerate}
  \item $\uSigma_{1} = \uI_{p \times p} $ is the identity matrix.
  \item $\uSigma_{2} $ is block diagonal matrix, with block $\uB = 0.85\uI_{25 \times 25} + 0.15\uJ_{25 \times 25}$. $\uJ$ denotes a matrix with 1 in all of its entries.
  \item $\uSigma_{3}$ is a diagonal matrix where the $20\%$ of the entries of the diagonal elements are $\sigma^2_j = 0.2p/j$ for $j = 1, \cdots, 0.2p$ and the remaining $\sigma^2_j = 1$ for $j > 0.2p$.
  \item $\uSigma$ is an AR(1) covariance matrix with $\sigma_{ij}=\sigma^2\rho^{|i - j|}\bone(|i-j| < 2)$. We chose $\sigma^2 = 1$ and $\rho = 0.4$.
  \item $\uSigma_{5}$ is an AR(1) covariance matrix with $\sigma_{ij}=\sigma^2\rho^{|i - j|}$. We chose $\sigma^2 = 1$ and $\rho = 0.6$.
%  \item $\uSigma_5$ is an ARIMA(1,1) covariance matrix with $\sigma_{ij}=\sigma^{2}\gamma^{1\{|i-j|>0\}}\rho^{|i-j| 1\{|i-j| \geq 2\}}$.
 % \item $\uSigma_5 = \uD^{1/2} \diag{\bone{k}} \bigotimes (.2\diag(\bone{2} + 0.8$
  %We chose $\sigma^{2} = 1$, $\gamma = 0.5$, $\rho = 0.9$.
\end{enumerate}
For each case, we also consider two possible alternatives. We simulate the non-all zeros mean vectors as follow
\begin{enumerate}[label=(\alph*)]
 \item[{\bf Alt.2:}]  $\umu_g \sim \uN_{p}(\bf{1}, \uI)$, set $p_0$ randomly selected elements to zero, and re-scale $\umu_g$ so that $\umu_g \trans\uSigma ^{-1}  \umu_g = 2$ 
 \item[{\bf Alt.1:}] $\umu_g \sim \uN_{p}(\bf{1}, \uI)$ and set $p_0$ of its elements to zero and re-scale $\umu_g$ so that $\frac{||\umu_{g}||^{2}}{\sqrt{\trace\left( \uSigma ^{2}\right)}} = 0.1$.
\end{enumerate}
The two alternatives described above were also considered by \citet{srivastava2014raptt,zoh2018powerful}.

\subsection{Simulation Results}
We first look at the performance of the both tests ($BF^{P}$ and $BF^{I}$) in term of their empirical power for simulation {\bf case 1} under alternative 1 (See Tables~\ref{tab:table1},~\ref{tab:table2}). 
Overall both tests tended to have an empirical type 1 error estimate around $5\%$; although in  some case the estimated type error seems slight inflated for the case of complex covariance matrices. 
We note a significant difference between both tests in term of the estimated empirical power. 
For the same setting, now looking at {\bf case 2}, we observations made in {\bf case1} still hold (see Tables S.1 ans S.2 from the Supplemental Material), except we observe a higher estimated power for the test on $BF_{I}$. Recall that in case 2, only the last group non-zero mean vector. 
The test based on the paired groups($BF_{I}$) performs much better when compared to the test based on the pooled covariance for data simulated under the alternative 1. 
Note that the data were simulated under the common covariance model. 
However, for data simulated under Alternative 2, again assuming common group covariance matrices, we see that both the tests based on pooled covariance ($BF_p$) and the test based on the pairwise groups ($BF_I$) performs very similarly. Although the test based on $BF_{p}$ tended to have slightly higher power and estimated Type 1 error near $0.05$ (see Tables~\ref{tab:table3}~\ref{tab:table4}).

%Alternative 1 and Group 3, case1
%Alternative 1 and Group 3, case1 (Pairwise BF)
\begin{table}[ht]
\centering
\caption{Alternative 1 and Group 3, case1 (Pairwise BF)}
\label{tab:table1}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|rr|rrrrrr|rrrlll|} \hline
  &&\multicolumn{6}{c|}{ $R_1$} & \multicolumn{6}{c|}{ $R_2$} \\ \hline
   & cp &  1 & 0.99 & 0.95 & 0.8 & 0.75 & 0.5 & 1 & 0.99 & 0.95 & 0.8 & 0.75 & 0.5 \\ 
  \hline
    \multirow{6}{*}{\rotatebox[origin=c]{90}{$n=50, p=200$}}
  & 1 &  0.033 & 0.833 & 0.711 & 0.675 & 0.633 & 0.658 &  0.033 & 0.833 & 0.711 & 0.675 & 0.633 & 0.658 \\ 
  & 2 &  0.034 & 0.796 & 0.654 & 0.666 & 0.616 & 0.668 &  0.034 & 0.796 & 0.654 & 0.666 & 0.616 & 0.668 \\ 
    & 3 &  0.038 & 0.780 & 0.643 & 0.610 & 0.586 & 0.589 & 0.038 & 0.780 & 0.643 & 0.610 & 0.586 & 0.589 \\ 
    & 4 &  0.049 & 0.494 & 0.408 & 0.396 & 0.402 & 0.451 &  0.049 & 0.494 & 0.408 & 0.396 & 0.402 & 0.451 \\ 
   & 5 &  0.064 & 0.437 & 0.347 & 0.343 & 0.329 & 0.375 &  0.064 & 0.437 & 0.347 & 0.343 & 0.329 & 0.375 \\ 
    & 6 &  0.073 & 0.283 & 0.235 & 0.224 & 0.217 & 0.237 &  0.073 & 0.283 & 0.235 & 0.224 & 0.217 & 0.237 \\ 
  \cline{3-14} \\
  \cline{3-14}
     \multirow{6}{*}{\rotatebox[origin=c]{90}{$n=70,p=1000$}}&  
     1 & 0.021 & 0.340 & 0.321 & 0.305 & 0.310 & 0.288 &  0.021 & 0.340 & 0.321 & 0.305 & 0.310 & 0.288 \\ 
   & 2 & 0.031 & 0.322 & 0.278 & 0.286 & 0.293 & 0.331 &  0.031 & 0.322 & 0.278 & 0.286 & 0.293 & 0.331 \\ 
    & 3 &  0.062 & 0.292 & 0.262 & 0.237 & 0.237 & 0.233 & 0.062 & 0.292 & 0.262 & 0.237 & 0.237 & 0.233 \\ 
    & 4 &  0.036 & 0.179 & 0.174 & 0.157 & 0.170 & 0.203 & 0.036 & 0.179 & 0.174 & 0.157 & 0.170 & 0.203 \\ 
     & 5 &  0.065 & 0.179 & 0.180 & 0.151 & 0.160 & 0.192 &  0.065 & 0.179 & 0.180 & 0.151 & 0.160 & 0.192 \\ 
   & 6 & 0.047 & 0.143 & 0.126 & 0.104 & 0.119 & 0.134 &  0.047 & 0.143 & 0.126 & 0.104 & 0.119 & 0.134 \\ 
   \hline
\end{tabular}
}
\end{table}

%----- alternative 1 and Group 3
%Alternative 1,  Group=3, case1 (Common Covariance)
\begin{table}[ht]
\centering
\caption{Alternative 1, Group=3, case1 (Common Covariance)}
\label{tab:table2}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|rr|rrrrrr|rrrlll|} \hline
 & &\multicolumn{6}{c|}{ $R_1$} & \multicolumn{6}{c|}{ $R_2$} \\ \hline
   & cp &  1 & 0.99 & 0.95 & 0.8 & 0.75 & 0.5 & 1 & 0.99 & 0.95 & 0.8 & 0.75 & 0.5 \\ 
  \hline
    \multirow{6}{*}{\rotatebox[origin=c]{90}{$n=50, p=200$}} 
 & 1 &   0.034 & 0.050 & 0.042 & 0.033 & 0.051 & 0.045  & 0.034 & 0.050 & 0.042 & 0.033 & 0.051 & 0.045 \\ 
  & 2 &   0.031 & 0.059 & 0.049 & 0.051 & 0.058 & 0.043  & 0.031 & 0.059 & 0.049 & 0.051 & 0.058 & 0.043 \\ 
 & 3 &   0.038 & 0.039 & 0.037 & 0.035 & 0.042 & 0.044  & 0.038 & 0.039 & 0.037 & 0.035 & 0.042 & 0.044 \\ 
   & 4 &   0.049 & 0.064 & 0.053 & 0.067 & 0.059 & 0.055  & 0.049 & 0.064 & 0.053 & 0.067 & 0.059 & 0.055 \\ 
  & 5 &   0.063 & 0.076 & 0.068 & 0.074 & 0.076 & 0.067  & 0.063 & 0.076 & 0.068 & 0.074 & 0.076 & 0.067 \\ 
  & 6 &   0.056 & 0.066 & 0.065 & 0.064 & 0.072 & 0.065  & 0.056 & 0.066 & 0.065 & 0.064 & 0.072 & 0.065 \\ 
   \cline{3-14} \\
  \cline{3-14}
    \multirow{6}{*}{\rotatebox[origin=c]{90}{$n=70,p=1000$}} 
  & 1 &   0.024 & 0.025 & 0.027 & 0.026 & 0.033 & 0.026  & 0.024 & 0.025 & 0.027 & 0.026 & 0.033 & 0.026 \\ 
  & 2 &   0.051 & 0.040 & 0.043 & 0.036 & 0.044 & 0.039  & 0.051 & 0.040 & 0.043 & 0.036 & 0.044 & 0.039 \\ 
  & 3 &   0.065 & 0.071 & 0.068 & 0.050 & 0.056 & 0.061  & 0.065 & 0.071 & 0.068 & 0.050 & 0.056 & 0.061 \\ 
   & 4 &   0.043 & 0.031 & 0.043 & 0.030 & 0.042 & 0.043  & 0.043 & 0.031 & 0.043 & 0.030 & 0.042 & 0.043 \\ 
   & 5 &   0.065 & 0.069 & 0.063 & 0.066 & 0.070 & 0.071  & 0.065 & 0.069 & 0.063 & 0.066 & 0.070 & 0.071 \\ 
  & 6 &   0.049 & 0.036 & 0.058 & 0.053 & 0.042 & 0.066  & 0.049 & 0.036 & 0.058 & 0.053 & 0.042 & 0.066 \\ 
   \hline
\end{tabular}
}
\end{table}

% latex table generated in R 4.1.0 by xtable 1.8-4 package
% Sat Dec  4 22:38:26 2021
% case1 - Alternative 2 - Group 3 (Pairwise BF)
\begin{table}[ht]
\centering
\caption{case1 - Alternative 2 - Group 3 (Pairwise BF)}
\label{tab:table3}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|rr|rrrrrr|rrrlll|} \hline
 & &\multicolumn{6}{c|}{ $R_1$} & \multicolumn{6}{c|}{ $R_2$} \\ \hline
   & cp &  1 & 0.99 & 0.95 & 0.8 & 0.75 & 0.5 & 1 & 0.99 & 0.95 & 0.8 & 0.75 & 0.5 \\ 
  \hline
    \multirow{6}{*}{\rotatebox[origin=c]{90}{$n=50, p=200$}} 
 & 1 &  0.033 & 0.570 & 0.450 & 0.429 & 0.408 & 0.408 &  0.033 & 0.570 & 0.450 & 0.429 & 0.408 & 0.408 \\ 
  & 2 & 0.034 & 0.790 & 0.637 & 0.598 & 0.537 & 0.508 & 0.034 & 0.790 & 0.637 & 0.598 & 0.537 & 0.508 \\
  & 3 &  0.038 & 0.989 & 0.997 & 1 & 0.996 & 0.998 &  0.038 & 0.989 & 0.997 & 1 & 0.996 & 0.998 \\ 
 & 4 &  0.049 & 0.716 & 0.603 & 0.563 & 0.518 & 0.518 &  0.049 & 0.716 & 0.603 & 0.563 & 0.518 & 0.518 \\ 
 & 5 & 0.064 & 0.944 & 0.843 & 0.802 & 0.755 & 0.706 &  0.064 & 0.944 & 0.843 & 0.802 & 0.755 & 0.706 \\ 
  & 6 & 0.073 & 0.869 & 0.770 & 0.717 & 0.694 & 0.680 &  0.073 & 0.869 & 0.770 & 0.717 & 0.694 & 0.680 \\ 
    \cline{3-14} \\
  \cline{3-14}
    \multirow{6}{*}{\rotatebox[origin=c]{90}{$n=70,p=1000$}}& 
 1 &  0.021 & 0.704 & 0.655 & 0.628 & 0.635 & 0.606 &  0.021 & 0.704 & 0.655 & 0.628 & 0.635 & 0.606 \\ 
  & 2 &  0.031 & 0.864 & 0.830 & 0.789 & 0.759 & 0.716 &  0.031 & 0.864 & 0.830 & 0.789 & 0.759 & 0.716 \\ 
  & 3 &  0.062 & 1 & 1 & 1 & 1 & 1 &  0.062 & 1 & 1 & 1 & 1 & 1 \\ 
 & 4 &  0.036 & 0.818 & 0.759 & 0.739 & 0.730 & 0.696 &  0.036 & 0.818 & 0.759 & 0.739 & 0.730 & 0.696 \\ 
 & 5 &  0.065 & 0.943 & 0.905 & 0.872 & 0.862 & 0.827 &  0.065 & 0.943 & 0.905 & 0.872 & 0.862 & 0.827 \\ 
 & 6 &  0.047 & 0.899 & 0.859 & 0.823 & 0.818 & 0.794 &  0.047 & 0.899 & 0.859 & 0.823 & 0.818 & 0.794 \\ 
   \hline
\end{tabular}
}
\end{table}

% latex table generated in R 4.1.0 by xtable 1.8-4 package
% Sat Dec  4 23:01:10 2021
%Alternative 2,  Group=3, case1 (Common Covariance BF)
\begin{table}[ht]
\centering
\caption{Alternative 2,  Group=3, case1 (Common Covariance BF)}
\label{tab:table4}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|rr|rrrrrr|rrrlll|} \hline
 & &\multicolumn{6}{c|}{ $R_1$} & \multicolumn{6}{c|}{ $R_2$} \\ \hline
 & cp &  1 & 0.99 & 0.95 & 0.8 & 0.75 & 0.5 & 1 & 0.99 & 0.95 & 0.8 & 0.75 & 0.5 \\ 
  \hline
    \multirow{6}{*}{\rotatebox[origin=c]{90}{$n=50, p=200$}} 
  & 1 &  0.034 & 0.564 & 0.450 & 0.438 & 0.437 & 0.427  & 0.034 & 0.564 & 0.450 & 0.438 & 0.437 & 0.427 \\ 
 & 2 &  0.031 & 0.781 & 0.674 & 0.590 & 0.596 & 0.530  & 0.031 & 0.781 & 0.674 & 0.590 & 0.596 & 0.530 \\ 
 & 3 &  0.038 & 0.986 & 0.996 & 1 & 0.997 & 0.999  & 0.038 & 0.986 & 0.996 & 1 & 0.997 & 0.999 \\ 
 & 4 &  0.049 & 0.715 & 0.612 & 0.559 & 0.577 & 0.532  & 0.049 & 0.715 & 0.612 & 0.559 & 0.577 & 0.532 \\ 
 & 5 &  0.063 & 0.944 & 0.868 & 0.803 & 0.788 & 0.733  & 0.063 & 0.944 & 0.868 & 0.803 & 0.788 & 0.733 \\ 
 & 6 &  0.056 & 0.872 & 0.783 & 0.731 & 0.739 & 0.708  & 0.056 & 0.872 & 0.783 & 0.731 & 0.739 & 0.708 \\ 
   \cline{3-14} \\
  \cline{3-14}
    \multirow{6}{*}{\rotatebox[origin=c]{90}{$n=70,p=1000$}}
  & 1 &  0.024 & 0.738 & 0.662 & 0.664 & 0.650 & 0.659  & 0.024 & 0.738 & 0.662 & 0.664 & 0.650 & 0.659 \\ 
  & 2 &  0.051 & 0.894 & 0.817 & 0.811 & 0.812 & 0.775  & 0.051 & 0.894 & 0.817 & 0.811 & 0.812 & 0.775 \\ 
  & 3 &  0.065 & 1 & 1 & 1 & 1 & 1  & 0.065 & 1 & 1 & 1 & 1 & 1 \\ 
  & 4 &  0.043 & 0.839 & 0.772 & 0.764 & 0.776 & 0.748  & 0.043 & 0.839 & 0.772 & 0.764 & 0.776 & 0.748 \\ 
  & 5 &  0.065 & 0.947 & 0.916 & 0.889 & 0.910 & 0.859  & 0.065 & 0.947 & 0.916 & 0.889 & 0.910 & 0.859 \\ 
  & 6 &  0.049 & 0.910 & 0.864 & 0.833 & 0.854 & 0.839  & 0.049 & 0.910 & 0.864 & 0.833 & 0.854 & 0.839 \\ 
   \hline
\end{tabular}
}
\end{table}

We also look at the performance of both tests for the case of $5$ groups (see Tables S.3-S.6 of the Supplemental Material). The observation made for the case of 5 groups also hold.

In the second part of the simulation, we simulate data assuming different covariance matrices between the active group (non-zero) mean vector and the non-active group (all zeros) mean vector. Namely, in {\bf case 1} all groups are assumed to have an identity covariance and the last group $G$ is assumed to have one of the  covariance matrix $\uSigma_k$, for each $k = 1, \cdots, 6$ (see Table~\ref{tab:table5}). 
In {\bf case 2}, however, the active group (Group G) has an identity  covariance matrix and the  non-active groups have the same covariance matrix which is one of the $\uSigma_k$, for each $k = 1, \cdots, 6$. Under these settings, we see that the test statistic based on $BF_{p}$ (pooled covariance) is poorly calibrated when the covariance matrices are (very) different. So we do not discuss its estimated power here. We instead focus on the test based on $BF_{I}$ (see Table~\ref{tab:table6}). The test based on on the $BF_{I}$ seems to hold the nominal Type I error for the case small sample size, even though the estimated Type error 1 seems inflated for the covariance matrix very different from the identity matrix.

% latex table generated in R 4.1.0 by xtable 1.8-4 package
% Sat Dec  4 23:19:35 2021
\begin{table}[ht]
\centering
\caption{Alternative 2,  Group=3, case1 (Pairwise Covariance BF)}
\label{tab:table5}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|rr|rrrrrr|rrrlll|} \hline
 & & \multicolumn{6}{c|}{ $R_1$} & \multicolumn{6}{c|}{ $R_2$} \\ \hline
  & cp &  1 & 0.99 & 0.95 & 0.8 & 0.75 & 0.5 & 1 & 0.99 & 0.95 & 0.8 & 0.75 & 0.5 \\ 
  \hline
   \multirow{6}{*}{\rotatebox[origin=c]{90}{$n=50, p=200$}}
  & 1 &  0.030 & 0.551 & 0.435 & 0.410 & 0.392 & 0.392 &  0.030 & 0.551 & 0.435 & 0.410 & 0.392 & 0.392 \\ 
  & 2 &  0.033 & 0.745 & 0.608 & 0.562 & 0.504 & 0.505 &  0.033 & 0.745 & 0.608 & 0.562 & 0.504 & 0.505 \\ 
  & 3 &  0.039 & 0.997 & 1 & 1 & 0.999 & 1 &  0.039 & 0.997 & 1 & 1 & 0.999 & 1 \\ 
  & 4 &  0.041 & 0.671 & 0.544 & 0.538 & 0.476 & 0.485 &  0.041 & 0.671 & 0.544 & 0.538 & 0.476 & 0.485 \\ 
  & 5 &  0.053 & 0.845 & 0.738 & 0.700 & 0.651 & 0.631 &  0.053 & 0.845 & 0.738 & 0.700 & 0.651 & 0.631 \\ 
  & 6 &  0.086 & 0.980 & 0.942 & 0.925 & 0.915 & 0.914 &  0.086 & 0.980 & 0.942 & 0.925 & 0.915 & 0.914 \\
    \cline{3-14} \\
  \cline{3-14}
    \multirow{6}{*}{\rotatebox[origin=c]{90}{$n=70,p=1000$}}
  & 1 &  0.009 & 0.621 & 0.554 & 0.539 & 0.526 & 0.509 &  0.009 & 0.621 & 0.554 & 0.539 & 0.526 & 0.509 \\ 
  & 2 &  0.015 & 0.811 & 0.763 & 0.714 & 0.708 & 0.667 &  0.015 & 0.811 & 0.763 & 0.714 & 0.708 & 0.667 \\ 
  & 3 &  0.059 & 1 & 1 & 1 & 1 & 1 &  0.059 & 1 & 1 & 1 & 1 & 1 \\ 
  & 4 &  0.011 & 0.738 & 0.692 & 0.659 & 0.659 & 0.617 &  0.011 & 0.738 & 0.692 & 0.659 & 0.659 & 0.617 \\ 
  & 5 &  0.034 & 0.916 & 0.864 & 0.838 & 0.845 & 0.810 &  0.034 & 0.916 & 0.864 & 0.838 & 0.845 & 0.810 \\ 
  & 6 &  0.150 & 1 & 0.999 & 0.998 & 0.999 & 0.997 &  0.150 & 1 & 0.999 & 0.998 & 0.999 & 0.997 \\ 
   \hline
\end{tabular}
}
\end{table}

%\subsection{Alternative 2,  Group=3, case2 (Pairwise Covariance BF)}
% latex table generated in R 4.1.0 by xtable 1.8-4 package
% Sat Dec  4 23:22:00 2021
\begin{table}[ht]
\centering
\caption{Alternative 2,  Group=3, case2 (Pairwise Covariance BF)}
\label{tab:table6}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|rr|rrrrrr|rrrlll|} \hline
 & &\multicolumn{6}{c|}{ $R_1$} & \multicolumn{6}{c|}{ $R_2$} \\ \hline
 & cp &  1 & 0.99 & 0.95 & 0.8 & 0.75 & 0.5 & 1 & 0.99 & 0.95 & 0.8 & 0.75 & 0.5 \\ 
  \hline
   \multirow{6}{*}{\rotatebox[origin=c]{90}{$n=50, p=200$}}
  & 1 &   0.030 & 0.921 & 0.866 & 0.782 & 0.781 & 0.712 &  0.030 & 0.921 & 0.866 & 0.782 & 0.781 & 0.712 \\ 
 & 2 &   0.033 & 0.993 & 0.966 & 0.922 & 0.931 & 0.881 &  0.033 & 0.993 & 0.966 & 0.922 & 0.931 & 0.881 \\ 
  & 3    & 0.039 & 1 & 1 & 1 & 1 & 1 &  0.039 & 1 & 1 & 1 & 1 & 1 \\ 
   & 4   & 0.041 & 0.979 & 0.952 & 0.885 & 0.910 & 0.821 &  0.041 & 0.979 & 0.952 & 0.885 & 0.910 & 0.821 \\ 
   & 5   & 0.053 & 1 & 0.995 & 0.992 & 0.986 & 0.970 &  0.053 & 1 & 0.995 & 0.992 & 0.986 & 0.970 \\ 
 & 6 & 0.086 & 1 & 0.997 & 0.994 & 0.994 & 0.983 &  0.086 & 1 & 0.997 & 0.994 & 0.994 & 0.983 \\ 
      \cline{3-14} \\
  \cline{3-14}
    \multirow{6}{*}{\rotatebox[origin=c]{90}{$n=70,p=1000$}}
     & 1   & 0.009 & 0.989 & 0.981 & 0.956 & 0.960 & 0.906 &  0.009 & 0.989 & 0.981 & 0.956 & 0.960 & 0.906 \\
  & 2   & 0.015 & 0.999 & 0.999 & 0.995 & 0.994 & 0.973 &  0.015 & 0.999 & 0.999 & 0.995 & 0.994 & 0.973 \\ 
   & 3    & 0.059 & 1 & 1 & 1 & 1 & 1 &  0.059 & 1 & 1 & 1 & 1 & 1 \\ 
    & 4   & 0.011 & 0.996 & 0.993 & 0.989 & 0.980 & 0.966 &  0.011 & 0.996 & 0.993 & 0.989 & 0.980 & 0.966 \\ 
   & 5   & 0.034 & 1 & 1 & 1 & 1 & 1 &  0.034 & 1 & 1 & 1 & 1 & 1 \\ 
   & 6   & 0.150 & 1 & 1 & 1 & 1 & 1 &  0.150 & 1 & 1 & 1 & 1 & 1 \\ 
   \hline
\end{tabular}
}
\end{table}

\section{Application} \label{sec:Application}
The data set used in our application originated from a head and neck squamous cell carcinoma (HNSCC) study where the profile of $5902$ single cells from 18 patients with oral cavity tumors by single cell RNA-seq are obtained. See \citep{puram2017single} for the detailed description of the study designed. The data set used for our analysis can be downloaded from the Gene expression Omnibus (\textbf{https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE103322}). Subsequently, each of the $5902$ cells are identified and labelled %(see table for the breakdown).
We would like to know if there is evidence that cell types have different gene expression profile while accounting for the potential dependency between the genes. We frame this problem as a high-dimensional mean vector testing. Under the NULL hypothesis, all cell types have equal or similar mean gene expression profile. Before we can apply our proposed test, we perform a feature reduction step to root out lowly expressed genes. Similar to the approach considered by \cite{puram2017single}, we choose genes with $E_a(i) > 6$, where $E_a(i)=\log2\text{(average(TPM(i)1...k)+1)}$. This results in $p = 2641$ genes selected. We consider the less popular (Tumor free) cell types (3 types): B-cells (n=138),  Macrophage (n=98), and Mast(n=120). To perform our test, we select the dimension of the projections space, according to our discussion above, to be $m = 88$, and the evidence threshold is set at $\gamma = 6.340$, and $\alpha = 0.05$. Since our test is sensible to significant departure of assumption of common covariance, we perform a test comparing covariance matrices (\citealp{ahmad2017location,srivastava2010testing}). Interesting enough, the test proposed by \citealt{ahmad2017location} reject the NULL hypothesis ($p-value < 0.001$). But the covariance test proposed by \cite{srivastava2010testing} does not reject the NULL ($p-value = 0.1878$). 
Finally, applying our test to the data set, we reject the NULL hypothesis with a $p-value < 0.001$ based on the NULL hypothesis of the test statistic in (\ref{eq:BFensbl}) approximated assuming zero mean vectors for each group and a common identity covariance matrix. In all, the test based on Bayes factor assuming common over all covariance matrix ($BF^{p}$) and the test based on pairwise Bayes factor $(BF^{I})$ both yield the same result for the projection matrices $\uR_1$ and $\uR_2$. 
Our testing procure also provides an automatic way to extract information about all pairwise comparisons. Note that information about each $f$ statistic is retained. A useful summary statistic we can look at is the proportions of $f$ statistic that exceeded the threshold of significance across all random projections (see table~\ref{tab:table7a}). We note that the test based assuming common covariance across all group $BF^{P}$ tended to have a larger proportion of significant test for all pairwise comparison when compared to its $BF^{I}$ counterpart.  

\begin{table}[ht]
\centering
%\caption{Cell type breakdown from \cite{puram2017single}. }
\label{tab:table7a}
\caption{Proportion of pairwise tests that were declared significant across $1000$ random projections. The results are reported for the both tests $(BF^{I}, BF^{P})$ .}
\begin{tabular}{|rrr|}
  \hline
   & Macrophage & B Cell \\
    Mast & (0.763, 0.943)  & (0.689, 0.819) \\
    Macrophage & - & (0.987, 1.0)  \\
  \hline
\end{tabular}
\end{table}

%   \text{Mat}_{\varphi\text{ to }M} = \kbordermatrix{
%     & Macrophage & B Cell \\
%     Mast & 0.763, 0.943)  & (0.689, 0.819) \\
%     Macrophage & - & (0.987, 1.0)  \\
%   %  B Cell & 0 & 0  \\
%   }
% \]

%$$

%Endothelial(n=260),
% latex table generated in R 4.1.0 by xtable 1.8-4 package
% Tue Nov 30 15:53:17 2021
% \begin{table}[ht]
% \centering
% \caption{Cell type breakdown from \cite{puram2017single}. }
% \label{tab:table7}
% \begin{tabular}{|rr|}
%   \hline
% Cell Names & Sample Sizes \\ 
%   \hline
% %Other Cells & 324 \\ 
%  % Fibroblast &  18 \\ 
%   B cell & 138 \\ 
%   %Dendritic &  51 \\ 
%   %Endothelial & 260 \\ 
%   %Fibroblast & 1422 \\ 
%   Macrophage &  98 \\ 
%   Mast & 120 \\ 
%  % myocyte &  19 \\ 
%  % T cell & 1237 \\ 
%  % tumor cell & 2215 \\ 
%   \hline
% \end{tabular}
% \end{table}

\section{Conclusion} \label{sec:conclusion}
When the dimension of the variables space exceeds or supersedes the sample sizes in each group, classical test statistic can no longer be directly employed and some regularization steps are needed. Here we consider the problem multiple group mean vector testing using random projections(RPs). We formulate two test based on Bayes factors which different by the assumption placed on the covariance matrix. One test statistics assume one overall covariance matrix which we denote as $BF^{P}$. The second test only assume pairwise common covariance matrix and is denoted as $BF^{I}$. When the assumption of homogeneous covariance matrix is reasonable, both test statistics perform similarly and very well. However, for moderate departure from the assumption of common covariance, the test based on $BF^{I}$ seems robust in the simulation setting we considered. Although, we should note that the test based on $BF^{I$ seems to have an inflated estimated type error in some settings.  A natural extension of this work is in the case of potentially very different covariance matrices. Additionally, the test statistic was derived assuming a normal distribution, it will be interesting to relax that potentially constraining assumption.           

%We show that RPs 
%continue to be the object of significant research related to h 

\section{Theoretical Justifications} \label{sec:theori}
We assume the following conditions.
\begin{description}
  \item[Assumption 1] $n_g/\sum_{g=1}^{G}n_g \rightarrow c \in (0, 1)$
  \item[Assumption 2] $G / \min\{ n_1,n_2, \cdots, n_G \} \rightarrow 0$ as $\min\{ n_1, \cdots, n_G \} \rightarrow \infty $
  \item[Assumption 3] $\max\{ n_1, n_2, \cdots, n_G \}/p \rightarrow \in (0, 1)$ 
  \item[Assumption 4] We denote a $m_n$, $\tau_{ij,n}$, and $\gamma_{ij,n}$ the values of $m$, $\tau_{0, ij}$ and $\gamma$ we derived in session~\ref{sec:testmtaugam}. Further, we assume that $m_n/n \rightarrow \theta \in(0, 1)$ so that $m_n n_{0,n}/\tau_{ij,n}} \rightarrow \infty$ as $n_{\min} \rightarrow \infty$. These conditions are satisfied by our construction (\citealp[see Appendix C of][]{zoh2018powerful}). 
\end{description}

\subsection{Consistency of $BF^{P}$}
\begin{Th}
Suppose that $X_{ig} = \umu_i + \uepsilon_{ig}$, where $\uepsilon_{ig} \sim \MVN_{p}(\bzero, \uSigma)$, for $g = 1, \cdots, G$ independent groups and $p >> \max\{n_1, \cdots, n_{G}\}$, where $n_1, \cdots, n_{G}$ are the respective sample sizes. Suppose $1 \leq m  \leq \min\{n_1, \cdots, n_{G}\}$ and $n = \sum^{G}_{g=1}n_g$. Let's $BF^{P}_{max} = \max\{BF^{P}_{12}, \cdots, BF^{P}_{(G-1)G}\}$, where $BF^{P}_{ij}$ is the BF based on the group $i$ and $j$ data. As $\min\{n_1, \cdots, n_{G}\} \rightarrow \infty$,
\begin{enumerate}
    \item If $m$ is fixed, then $\log(BF^{P}_{max}) \rightarrow -\infty$ under $H_0$ and $\log(BF^{P}_{max}) \rightarrow \infty$ under $H_1$.
    \item If $m(n)$ so that $m(n)/n \rightarrow \theta \in (0, 1)$, then $\log(BF^{P}_{max}) = \mathcal{O}(n)$ under $H_0$ and $\log(BF^{P}_{max}) \rightarrow \infty$ under as sequence of alternative $H_{1n}$.
\end{enumerate}
\end{Th}

\subsubsection{Proofs}
\begin{description}
\item[Part(1)]
For $1 < m < \min\{ n_1, \cdots, n_{G} \}$ and $n = \sum^{G}_{g}n_g$, we integrate out the parameters with respect to the conjugate priors to obtain the Bayes factor in favor of the alternative as
\be
BF^{P}_{ij}(\uR) &=&\left(1 + \eta \right)^{-m/2} \left\{ 1 -  \frac{ m f_{ij}^{\star} \eta/(1 + \eta) }{ m f_{ij}^{\star}  + n - m - (G-1)} \right\}^{-(n-1)/2} \nonumber,
\ee
where
\bse
f_{ij}^{\star}  = \frac{n - m - (G-1)}{(n-G)m} n_{0,ij}(\overline{\uY} - \overline{\uX})\trans \uR(\uR \uS\uR\trans )^{-1}\uR\trans (\overline{\uY} - \overline{\uX}).
\ese
where $$ \uS = \frac{1}{n - G} \sum^{G}_{g=1}(n_g -1)\uS_{g}}\; \text{and} \;\; \uS_g = \frac{1}{n_g - 1}\sum^{n_g}_{i=1}(\uX_ig - \overline{\uX}_{g})(\uX_ig - \overline{\uX}_{g})\trans.$$
Recall that $1/n_{0,ij} = 1/n_i + 1/n_j$, $\eta_{ij} = n_{0,ij}/\tau_{0,ij}$, and $n_{\min} = \min\{n_1, \cdots, n_G\}$.
Since $\tau_{0,ij}$ is fixed, $\eta_{ij} \rightarrow \infty$ as $n_{\min} \to \infty$.
For a randomly chosen $\uR$, under $H_{0}$, $f_{ij}^{\star}  \sim F_{m, n-m-(G-1)}$ with $m$ and $n - m -(G-1)$ degrees of freedom.
Thus, $f_{ij}^{\star} = O_{p}(1)$ and $f^{max}_{ij} = \max_{ij}\{ f_{ij}\}$ for all $1 \leq i, j \leq G$.
Also, from well-known properties of the $F$ distribution, we have that
\bse
U_{ij} = \frac{m f_{ij}^{\star}/(n-m-(G-1) )}{\{m f_{ij}^{\star}/(n-m- (G-1) )\}} = \nonumber \\
  \frac{m f_{ij}^{\star}}{( m f_{ij}^{\star} +n-m- (G-1))} \sim \Beta\{m/2, (n-m-(G-1))/2 \},
\ese
for each $ij$ with $1 \leq i, j \leq G$, where $\Beta(a, b)$ denotes a Beta distribution.
Therefore, $\{\eta_{ij}/(1 + \eta_{ij})\} U_{ij} = O_{p}(1)$.
Hence, $\log\left\{ 1 -  \eta_{ij} U_{ij}/(1 + \eta_{ij}) \right\} = O_{p}(1)$ as $n_{\min} \to \infty$. We then get
\bse
-\frac{m}{2n}\log(1 + \eta_{ij}) -\frac{(n-1)}{2n}\log\left\{ 1 -  \eta_{ij} U_{ij}/(1 + \eta) \right\} \xrightarrow[]{p} -\infty,
\ese
since $\log(1 + \eta_{ij}) \rightarrow \infty$ as $n_{\min} \rightarrow \infty$ and $\lim_{n_{\min} \rightarrow \infty} m/n = \theta \in (0, 1)$.
We conclude that $\log\{BF^{P}_{ij}(\uR)\} \xrightarrow[]{p} -\infty$ under the null hypothesis for all $1 \leq i < j \leq G$.
Hence $\log\{BF^{P}_{10}(\uR) \xrightarrow[]{p} -\infty$.

Under the alternative, $\umu_{i} \neq \umu_{j}$ and $\udelta_{ij} \sim \uN_{p}({\bf 0}, \uSigma /\tau_{0})$.
Then, $f_{ij}^{\star} \mid   \lambda_{ij} \sim F_{m, n-m-(G-1)}(\lambda_{ij})$
with non-centrality $\lambda_{ij} = n_{0,ij}\udelta_{ij}\trans\uR(\uR\trans \uSigma \uR)^{-1}\uR\trans \udelta_{ij}$.
Since $\udelta_{ij} \sim \uN_{p}({\bf 0}, \uSigma /\tau_{0})$, $\lambda \sim n_{0,ij}\chi_{m}^{2}/\tau_{0}$,
where $\chi_{m}^{2}$ denotes a $\chi^{2}$ distribution with m degrees of freedom. The non-centrality parameter depends on $n$ through $n_{0,ij}$.
We can show that the unconditional distribution of $f_{ij}^{\star} /(1 + \eta_{ij}) \sim F_{m, n-m-(G-1)}$ \citep[see][page 704]{johnson2005bayes}.
If we denote $f_{ij}^{0} =  f_{ij}^{\star} /(1 + \eta_{ij})$, we have $f_{ij}^{0} = O_{p}(1)$, and $mf_{ij}^{0}/n = O_{p}(1)$, as $n_{\min} \rightarrow \infty$.
We have that
\be
\frac{\eta_{ij} U_{ij}}{(1+\eta_{ij})} = \frac{ m f_{ij}^{0}\eta_{ij}}{m f_{ij}^{0}(1+\eta_{ij})+n-m-(G-1)} \nonumber
\ee
From the above equation, we get
\be
-\log\left\{ 1 - \frac{\eta_{ij} U_{ij}}{(1+\eta_{ij})} \right \} &=& \log\left\{ \frac{mf_{ij}^{0}(1+\eta_{ij})/(n-m-(G-1)) + 1}{\{mf_{ij}^{0}/(n-m-(G-1))\} +1 }\right\}. \nonumber
\ee
Since $f_{ij}^{0} = O_{p}(1)$, and $m/(n-m-1)$ converges, we have
\bse
 -\log\left\{ 1 - \frac{\eta_{ij} U_{ij}}{(1+\eta_{ij})} \right \} \xrightarrow[]{p} \infty.
\ese
Since this is true for all $(i,j)$ with $\umu_i \neq \umu_j$ with $1 \leq i < j \leq G$,  we conclude that $\log\left\{BF^{P}_{10}(\uR) \right \} \xrightarrow[]{p} \infty$, under the alternative hypothesis.

\item[Part(2)]
We now assume that $\eta_{ij} \rightarrow 0$ and $m_n \eta_{ij} \rightarrow \infty$. We have
\be
 \log\{BF^{P}_{10}(\uR)\} =\frac{n}{2}\left( 1 - \frac{m_n}{n}\right)\log\left(1 + \eta_{ij} \right) - \frac{n}{2}\log\left\{ 1 + \eta_{ij}(1-U_{ij}) \right \} +\frac{1}{2}\log \left\{1 - \frac{\eta_{ij} U_{ij}}{1+\eta_{ij}}\right\},  \nonumber
\ee
where $U_{ij} \sim Beta\left\{ m/2,  (n - m_n -(G-1))/2 \right\}$ under $H_0$ for each $1 \leq i < j \leq G$.
For large $n$, none of the terms with $n$ dominates and their difference converges.
The distribution $ \log\{BF_{10}(\uX^{\star},\uY^{\star}) \}$ then depends on that of $U$, which is bounded in probability.
Therefore, under $H_0$, $\log\{BF^{P}_{10, ij}(\uR) \} = \mathcal{O}_{p}(1)$ and we conclude $\log\{BF^{P}_{10}(\uR) \} = \mathcal{O}_{p}(1)$ for each $1 \leq i < j \leq G$.

Under $H^{n}_{1}$, again we have
\be
\log\{BF^{P}_{10,ij}(\uR) \} = -\frac{m}{2}\log(1 + \eta) - \frac{(n-1)}{2}\log\left\{ 1 - \frac{\eta U}{1 + \eta} \right\}, \nonumber
\ee
where $U_{ij} \xrightarrow[]{p} 1$ with $f_{ij}^{*}\xrightarrow[]{p} \infty$.
Since $\log(1 + \eta_{ij})\{ (n-1)/2 - m_n/2\} \rightarrow \infty$, we conclude that $\log\{BF^{P}_{10,ij}(\uR) \} \xrightarrow[]{p} \infty $ for $(i,j)$ where $\umu_i \neq \umu_j$. Hence $\log\{BF^{P}_{10}(\uR) \} \xrightarrow[]{p} \infty $
\end{description}

\subsection{Consistency of $BF^{I}$}
\begin{Th}
Suppose that $X_{ig} = \umu_i + \uepsilon_{ig}$, where $\uepsilon{ig} \sim \MVN_{p}(\bzero, \uSigma)$, for $g = 1, \cdots, G$ independent groups and $p >> \max\{n_1, \cdots, n_{G}\}$, where $n_1, \cdots, n_{G}$ are the respective sample sizes. Suppose $1 \leq m  \leq \min\{n_1, \cdots, n_{G}\}$ and $n = \sum^{G}_{g=1}n_g$. Let's $BF^{P}_{max} = \max\{BF^{P}_{12}, \cdots, BF^{P}_{(G-1)G}\}$, where $BF^{P}_{ij}$ is the BF based on the group $i$ and $j$ data. As $\min\{n_1, \cdots, n_{G}\} \rightarrow \infty$, As $\min\{n_1, \cdots, n_{G}\} \rightarrow \infty$,
\begin{enumerate}
    \item If $m$ is fixed, then $\log(BF^{I}_{max}) \rightarrow -\infty$ under $H_0$ and $\log(BF^{I}_{max}) \rightarrow \infty$ under $H_1$ 
    \item If $m(n)$ so that $m(n)/n \rightarrow \theta \in (0, 1)$, then $\log(BF^{I}_{max}) = \mathbb{O}(n)$ under $H_0$ and $\log(BF^{I}_{max}) = \infty$ under a sequence of alternative $H_{1n}$.
\end{enumerate}
\end{Th}

\subsubsection{Proofs}
\begin{description}
\item[Part(1)]
For $1 < m < \min\{ n_1, \cdots, n_{G} \}$ and $n = \sum^{G}_{g}n_g$, we integrate out the parameters with respect to the conjugate priors to obtain the Bayes factor in favor of the alternative as
\be
BF^{I}_{10,ij}(\uR) &=&\left(1 + \eta_{ij} \right)^{-m/2} \left\{ 1 -  \frac{ m f_{ij}^{\star} \eta_{ij}/(1 + \eta_{ij}) }{ m f_{ij}^{\star}  + n_{ij} - m - 1} \right\}^{-(n_{ij}-1)/2} \nonumber,
\ee
where
\bse
f_{ij}^{\star}  = \frac{n_{ij} - m - 1}{(n_{ij}-2)m} n_{0,ij}(\overline{\uX}_i - \overline{\uX}_j)\trans \uR(\uR \uS\uR\trans )^{-1}\uR\trans (\overline{\uX}_{i} - \overline{\uX}_{j}).
\ese
where $$ \uS_{ij} = \frac{1}{n_{ij} - 2} \sum_{g=(i,j)}(n_j -1)\uS_{g}\; \text{and} \;\; \uS_g = \frac{1}{n_g - 1}\sum^{n_g}_{i=1}(\uX_ig - \overline{\uX}_{g})(\uX_ig - \overline{\uX}_{g})\trans.$$
Recall that $1/n_{0,ij} = 1/n_i + 1/n_j$, $\eta_{ij} = n_{0,ij}/\tau_{0,ij}$, and $n_{\min} = \min\{n_i, n_j\}$.
Since $\tau_{0,ij}$ is fixed, $\eta_{ij} \rightarrow \infty$ as $n_{\min} \to \infty$.
For a randomly chosen $\uR$, under $H_{0}$, $f_{ij}^{\star}  \sim F_{m, n_{ij}-m-1}$ with $m$ and $n_{ij} - m -1$ degrees of freedom.
Thus, $f_{ij}^{\star} = O_{p}(1)$ and $f^{max}_{ij} = \max_{ij}\{ f^{\star}_{ij}\}$ for all $1 \leq i, j \leq G$.
Also, from well-known properties of the $F$ distribution, we have that
\bse
U_{ij} = \frac{m f_{ij}^{\star}/(n_{ij}-m-1 )}{\{m f_{ij}^{\star}/(n_{ij} - m - 1 )\}} = \nonumber \\
  \frac{m f_{ij}^{\star}}{( m f_{ij}^{\star} + n_{ij} - m-1)} \sim \Beta\{m/2, (n_{ij}-m-1)/2 \},
\ese
for each $ij$ with $1 \leq i, j \leq G$, where $\Beta(a, b)$ denotes a Beta distribution.
Therefore, $\{\eta_{ij}/(1 + \eta_{ij})\} U_{ij} = O_{p}(1)$.
Hence, $\log\left\{ 1 -  \eta_{ij} U_{ij}/(1 + \eta_{ij}) \right\} = O_{p}(1)$ as $n_{\min} \to \infty$. We then get
\bse
-\frac{m}{2n_{ij}}\log(1 + \eta_{ij}) -\frac{(n_{ij}-1)}{2n_{ij}}\log\left\{ 1 -  \eta_{ij} U_{ij}/(1 + \eta_{ij}) \right\} \xrightarrow[]{p} -\infty,
\ese
since $\log(1 + \eta_{ij}) \rightarrow \infty$ as $n_{\min} \rightarrow \infty$ and $\lim_{n_{\min} \rightarrow \infty} m/n_{ij} = \theta \in (0, 1)$.
We conclude that $\log\{BF^{I}_{ij}(\uR)\} \xrightarrow[]{p} -\infty$ under the null hypothesis for all $1 \leq i < j \leq G$.
Hence $\log\{BF^{I}_{10}(\uR)\} \xrightarrow[]{p} -\infty$.

Under the alternative, $\umu_{i} \neq \umu_{j}$ and $\udelta_{ij} \sim \uN_{p}({\bf 0}, \uSigma /\tau_{0})$.
Then, $f_{ij}^{\star} \mid   \lambda_{ij} \sim F_{m, n_{ij}-m-1}(\lambda_{ij})$
with non-centrality $\lambda_{ij} = n_{0,ij}\udelta_{ij}\trans\uR(\uR\trans \uSigma \uR)^{-1}\uR\trans \udelta_{ij}$.
Since $\udelta_{ij} \sim \uN_{p}({\bf 0}, \uSigma /\tau_{0})$, $\lambda \sim n_{0,ij}\chi_{m}^{2}/\tau_{0}$,
where $\chi_{m}^{2}$ denotes a $\chi^{2}$ distribution with m degrees of freedom. The non-centrality parameter depends on $n_{ij}$ through $n_{0,ij}$.
We can show that the unconditional distribution of $f_{ij}^{\star} /(1 + \eta_{ij}) \sim F_{m, n_{ij}-m-1}$ \citep[see][page 704]{johnson2005bayes}.
If we denote $f_{ij}^{0} =  f_{ij}^{\star} /(1 + \eta_{ij})$, we have $f_{ij}^{0} = O_{p}(1)$, and $mf_{ij}^{0}/n_{ij} = O_{p}(1)$, as $n_{\min} \rightarrow \infty$.
We have that
\be
\frac{\eta_{ij} U_{ij}}{(1+\eta_{ij})} = \frac{ m f_{ij}^{0}\eta_{ij}}{m f_{ij}^{0}(1+\eta_{ij})+n_{ij}-m-1} \nonumber
\ee
From the above equation, we get
\be
-\log\left\{ 1 - \frac{\eta_{ij} U_{ij}}{(1+\eta_{ij})} \right \} &=& \log\left\{ \frac{mf_{ij}^{0}(1+\eta_{ij})/(n_{ij}-m-1) + 1}{\{mf_{ij}^{0}/(n-m-1)\} +1 }\right\}. \nonumber
\ee
Since $f_{ij}^{0} = O_{p}(1)$, and $m/(n_{ij}-m-1)$ converges, we have
\bse
 -\log\left\{ 1 - \frac{\eta_{ij} U_{ij}}{(1+\eta_{ij})} \right \} \xrightarrow[]{p} \infty.
\ese
Since this is true for all $(i,j)$ with $\umu_i \neq \umu_j$ with $1 \leq i < j \leq G$,  we conclude that $\log\left\{BF^{I}_{10}(\uR) \right \} \xrightarrow[]{p} \infty$, under the alternative hypothesis.

\item[Part(2)]
We now assume that $\eta_{ij} \rightarrow 0$ and $m_n \eta_{ij} \rightarrow \infty$. We have
\be
 \log\{BF^{I}_{10}(\uR)\} =\frac{n_{ij}}{2}\left( 1 - \frac{m_n}{n_{ij}}\right)\log\left(1 + \eta_{ij} \right) - \frac{n_{ij}}{2}\log\left\{ 1 + \eta_{ij}(1-U_{ij}) \right \} +\frac{1}{2}\log \left\{1 - \frac{\eta_{ij} U_{ij}}{1+\eta_{ij}}\right\},  \nonumber
\ee
where $U_{ij} \sim Beta\left\{ m/2,  (n_{ij} - m_n -(G-1))/2 \right\}$ under $H_0$ for each $1 \leq i < j \leq G$.
For large $n_{ij}$, none of the terms with $n_{ij}$ dominates and their difference converges.
The distribution of $\log\{BF^{I}_{10}(\uR) \}$ then depends on that of $U$, which is bounded in probability.
Therefore, under $H_0$, $\log\{BF^{I}_{10, ij}(\uR) \} = \mathcal{O}_{p}(1)$ and we conclude $\log\{BF^{P}_{10}(\uR) \} = \mathcal{O}_{p}(1)$ for all $1 \leq i < j \leq G$.

Under $H^{n}_{1}$, again we have
\be
\log\{BF^{I}_{10,ij}(\uR) \} = -\frac{m}{2}\log(1 + \eta) - \frac{(n_{ij}-1)}{2}\log\left\{ 1 - \frac{\eta U}{1 + \eta} \right\}, \nonumber
\ee
where $U_{ij} \xrightarrow[]{p} 1$ with $f_{ij}^{*}\xrightarrow[]{p} \infty$.
Since $\log(1 + \eta_{ij})\{ (n_{ij}-1)/2 - m_n/2\} \rightarrow \infty$, we conclude that $\log\{BF^{I}_{10,ij}(\uR) \} \xrightarrow[]{p} \infty $ for $(i,j)$ where $\umu_i \neq \umu_j$. Hence $\log\{BF^{I}_{10}(\uR) \} \xrightarrow[]{p} \infty $
\end{description}

\subsection{Power and Size of the test}
\begin{Th}
Suppose the assumptions of Theorems 1 and 2 hold.
Given a collection $\uR_1, \cdots, \uR_N$ of random projections matrices,
where $\uR_{i}\trans\uR_{i} = \uI$ for all $i = 1, \cdots, N$, then $\lim_{n_{\min} \rightarrow \infty} P\{\widetilde{\boldpsi}(N) > \widetilde{\boldpsi}^{0}_{\alpha}\}  = 1$ under the sequence $H^{n}_{1}$ of alternatives.
%\begin{description}
%\item[part(a)] Under $H_0$, $E\{\boldpsi}(N)\}  = \alpha$
%\item[part(b)] 
%\end{description}
\end{Th}
\subsubsection{Proofs}
The proof is similar to that of \citealp{zoh2018powerful}.
The power of our test is $P\{ \widetilde{\psi}(N) > \widetilde{\psi}^{0}_{\alpha}\mid  H^{n}_{1}\}$.
Henceforth, we make it explicit that $\widetilde{\psi}^{0}_{\alpha}$ depends on $(n_1, \cdots ,n_G)$ and write $\widetilde{\psi}^{0}_{\alpha}(n_1, \cdots, n_G)$ instead.

For given $n_1, \cdots, n_G$ and $\alpha$, we choose $\widetilde{\psi}^{0}_{\alpha}(n_1, n_2)$ so that $P\{\widetilde{\psi}(N) > \widetilde{\psi}^{0}_{\alpha}(n_1, n_2)\mid  H_{0}\}  = \alpha$.
Since $ 0 < \widetilde{\psi}^{0}_{\alpha}(n_1, n_2) < 1$, for $0 < \alpha < 1$, we have that $P\{ \sum^{N}_{i=1}\widetilde{\psi}(\uR_i) \geq             0 \mid H^{n}_{1} \} \geq P\{ \sum^{N}_{i=1}\widetilde{\psi}(\uR_i) > N\phi^{0}_{\alpha}(n_1, n_2) \mid H^{n}_{1} \} \geq P\{ \sum^{N}_{i=1}\widetilde{\psi}(\uR_i) \geq N \mid H^{n}_{1} \}$.

We have that $P\{\widetilde{\psi}(\uR_i) =1 \mid H^{n}_{1}\} \rightarrow 1$ as $n_{\min} \rightarrow \infty$, under the alternative for $i = 1, \cdots, N$. So, $P\{ \sum^{N}_{i=1}\phi(\uR_i) \geq 0 \mid H^{n}_{1} \} = 1 - \prod_{i=1}^{N}P\{\phi(\uR_i) = 0 \mid H^{n}_{1} \} \rightarrow 1$.
Additionally, $P\{ \sum^{N}_{i=1}\widetilde{\psi}(\uR_i) \geq N \mid H^{n}_{1} \} = P\{ \sum^{N}_{i=1}\widetilde{\psi}(\uR_i) = N \mid H^{n}_{1} \} = \prod_{i=1}^{N}P\{\widetilde{\psi}(\uR_i) = 1 \mid H^{n}_{1}\} \rightarrow 1$ for fixed $N$ as $n_{\min} \rightarrow \infty$. We conclude that $P\{ \widetilde{\psi}(N) \geq \phi^{0}_{\alpha}\mid  H^{n}_{1}\} \rightarrow 1$ as $n_{\min} \rightarrow \infty.$

\subsection{Invariance under the NULL (assuming equal $\uSigma$)}
%We claim that both Bayes factor based tests are invariant under the NULL hypothesis assuming common covariance matrix $\uSigma$.

\includegraphics[]{}
%\bibliographystyle{plainnat}
\bibliographystyle{apalike}
\bibliography{Bibliography_lrtnew.bib}

\end{document}








